"""
èµ„é‡‘æµå‘åˆ†æå™¨ - å¢å¼ºç‰ˆ
ä¼˜åŒ–ç®—æ³•å¹¶æ·»åŠ è¯¦ç»†è¯´æ˜
"""
import pandas as pd
import numpy as np
from typing import Optional, Tuple, Dict

class FlowAnalyzer:
    """
    èµ„é‡‘æµå‘åˆ†æå™¨
    
    ç®—æ³•è¯´æ˜ (Level-2 å¢å¼ºç®—æ³•):
    1. **æ•°æ®åŸºç¡€**: è·å–é€ç¬”æˆäº¤è®°å½•ï¼ˆæ—¶é—´ã€ä»·æ ¼ã€æˆäº¤é‡ã€ä¹°å–æ–¹å‘ï¼‰
    2. **èµ„é‡‘åˆ†çº§**: 
       - **ä¸»åŠ›èµ„é‡‘**: å•ç¬”æˆäº¤é¢ â‰¥ 20ä¸‡å…ƒ
       - **æ•£æˆ·èµ„é‡‘**: å•ç¬”æˆäº¤é¢ < 20ä¸‡å…ƒ
    3. **æµå‘è®¡ç®—**:
       - ä¸»åŠ›æµå…¥ = âˆ‘(ä¸»åŠ›çº§åˆ« & ä¸»åŠ¨ä¹°å…¥)
       - ä¸»åŠ›æµå‡º = âˆ‘(ä¸»åŠ›çº§åˆ« & ä¸»åŠ¨å–å‡º)
       - ä¸»åŠ›å‡€æµå…¥ = ä¸»åŠ›æµå…¥ - ä¸»åŠ›æµå‡º
    
    æ³¨æ„äº‹é¡¹ï¼š
    - æ­¤é˜ˆå€¼(20ä¸‡)ä¸ºé€šç”¨å‚è€ƒæ ‡å‡†ï¼Œä¸åŒè½¯ä»¶å¯èƒ½æœ‰ç»†å¾®å·®å¼‚
    """
    
    def __init__(self, large_order_threshold: float = 200000):
        """
        Args:
            large_order_threshold: å¤§å•é˜ˆå€¼ï¼ˆå…ƒï¼‰ï¼Œé»˜è®¤20ä¸‡ (Level-2 å¸¸ç”¨æ ‡å‡†)
        """
        self.large_order_threshold = large_order_threshold

    def _get_time_column(self, df: pd.DataFrame) -> Optional[str]:
        for time_col in ['æ—¶é—´', 'æˆäº¤æ—¶é—´', 'time', 'datetime', 'æ—¶é—´æˆ³']:
            if time_col in df.columns:
                return time_col
        return None

    def _infer_granularity(self, df: pd.DataFrame) -> str:
        time_col = self._get_time_column(df)
        if time_col:
            time_series = pd.to_datetime(df[time_col], errors='coerce').dropna().sort_values()
            if len(time_series) >= 2:
                deltas = time_series.diff().dt.total_seconds().dropna()
                if not deltas.empty:
                    median_sec = float(deltas.median())
                    if median_sec >= 45:
                        return "minute"
                    if median_sec <= 5:
                        return "tick"
        row_count = len(df)
        if row_count >= 1200:
            return "tick"
        if 100 <= row_count <= 400:
            return "minute"
        return "unknown"

    def _normalize_flow_columns(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[str], Dict]:
        df_copy = df.copy()
        meta: Dict = {
            "direction_source": "unknown",
            "data_granularity": "unknown",
        }

        if 'æˆäº¤é¢(å…ƒ)' not in df_copy.columns:
            if 'amount' in df_copy.columns:
                df_copy['æˆäº¤é¢(å…ƒ)'] = df_copy['amount']
            elif 'æˆäº¤é¢' in df_copy.columns:
                df_copy['æˆäº¤é¢(å…ƒ)'] = df_copy['æˆäº¤é¢']
            elif 'æˆäº¤é‡‘é¢' in df_copy.columns:
                df_copy['æˆäº¤é¢(å…ƒ)'] = df_copy['æˆäº¤é‡‘é¢']
            else:
                return df_copy, "Missing transaction amount data"

        df_copy['æˆäº¤é¢(å…ƒ)'] = pd.to_numeric(df_copy['æˆäº¤é¢(å…ƒ)'], errors='coerce').fillna(0)

        if 'æ€§è´¨' not in df_copy.columns:
            if 'type' in df_copy.columns:
                df_copy['æ€§è´¨'] = df_copy['type']
                meta["direction_source"] = "å­—æ®µæ˜ å°„"
            elif 'ä¹°å–ç›˜æ€§è´¨' in df_copy.columns:
                df_copy['æ€§è´¨'] = df_copy['ä¹°å–ç›˜æ€§è´¨']
                meta["direction_source"] = "å­—æ®µæ˜ å°„"
            elif 'price_change' in df_copy.columns:
                df_copy['æ€§è´¨'] = df_copy['price_change'].apply(
                    lambda x: 'ä¹°ç›˜' if x > 0 else ('å–ç›˜' if x < 0 else 'ä¸­æ€§ç›˜')
                )
                meta["direction_source"] = "ä»·æ ¼å˜åŒ–æ¨æ–­"
            elif 'æ”¶ç›˜' in df_copy.columns:
                df_copy['price_change'] = df_copy['æ”¶ç›˜'].diff().fillna(0)
                df_copy['æ€§è´¨'] = df_copy['price_change'].apply(
                    lambda x: 'ä¹°ç›˜' if x > 0 else ('å–ç›˜' if x < 0 else 'ä¸­æ€§ç›˜')
                )
                meta["direction_source"] = "ä»·æ ¼å˜åŒ–æ¨æ–­"
            elif 'æˆäº¤ä»·æ ¼' in df_copy.columns:
                df_copy['price_change'] = df_copy['æˆäº¤ä»·æ ¼'].diff().fillna(0)
                df_copy['æ€§è´¨'] = df_copy['price_change'].apply(
                    lambda x: 'ä¹°ç›˜' if x > 0 else ('å–ç›˜' if x < 0 else 'ä¸­æ€§ç›˜')
                )
                meta["direction_source"] = "ä»·æ ¼å˜åŒ–æ¨æ–­"
            else:
                df_copy['æ€§è´¨'] = 'ä¸­æ€§ç›˜'
                meta["direction_source"] = "é»˜è®¤ä¸­æ€§"
        else:
            meta["direction_source"] = "åŸå§‹ä¹°å–æ–¹å‘"

        meta["data_granularity"] = self._infer_granularity(df_copy)
        return df_copy, None, meta

    def _get_large_order_threshold(self, df: pd.DataFrame, granularity: str) -> Tuple[float, str]:
        if granularity == "minute":
            quantile_threshold = float(df['æˆäº¤é¢(å…ƒ)'].quantile(0.9))
            if np.isnan(quantile_threshold):
                return self.large_order_threshold, "fixed_fallback"
            return max(self.large_order_threshold, quantile_threshold), "quantile_90_or_fixed"
        return self.large_order_threshold, "fixed"

    def calculate_flow_series(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç”Ÿæˆé€ç¬”å‡€æµå…¥ä¸ç´¯è®¡å‡€æµå…¥åºåˆ—ï¼ˆç”¨äºå›¾è¡¨å¯¹æ¯”ï¼‰
        """
        if df.empty:
            return df.copy()

        df_flow, error, _meta = self._normalize_flow_columns(df)
        if error:
            return pd.DataFrame()

        if 'æ—¶é—´' not in df_flow.columns:
            for time_col in ['æˆäº¤æ—¶é—´', 'time', 'datetime', 'æ—¶é—´æˆ³']:
                if time_col in df_flow.columns:
                    df_flow = df_flow.rename(columns={time_col: 'æ—¶é—´'})
                    break

        if 'æ—¶é—´' in df_flow.columns:
            df_flow['æ—¶é—´'] = pd.to_datetime(df_flow['æ—¶é—´'], errors='coerce')
            df_flow = df_flow.dropna(subset=['æ—¶é—´']).sort_values('æ—¶é—´')

        nature = df_flow['æ€§è´¨'].astype(str)
        df_flow['å‡€æµå…¥é¢'] = 0.0
        df_flow.loc[nature.str.contains('ä¹°'), 'å‡€æµå…¥é¢'] = df_flow['æˆäº¤é¢(å…ƒ)']
        df_flow.loc[nature.str.contains('å–'), 'å‡€æµå…¥é¢'] = -df_flow['æˆäº¤é¢(å…ƒ)']
        df_flow['ç´¯è®¡å‡€æµå…¥'] = df_flow['å‡€æµå…¥é¢'].cumsum()

        return df_flow
    
    def calculate_flows(self, df: pd.DataFrame) -> dict:
        """
        è®¡ç®—èµ„é‡‘æµå‘
        
        Returns:
            åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
            - total_turnover: æ€»æˆäº¤é¢
            - large_order_net_inflow: ä¸»åŠ›å‡€æµå…¥
            - retail_net_inflow: æ•£æˆ·å‡€æµå…¥
            - ...
        """
        if df.empty:
            return {}

        df, error, meta = self._normalize_flow_columns(df)
        if error:
            return {"error": error}

        granularity = meta.get("data_granularity", "unknown")
        threshold, threshold_note = self._get_large_order_threshold(df, granularity)

        # 1. åˆ’åˆ†èµ„é‡‘ç±»å‹ (æ ¹æ®é˜ˆå€¼)
        # ä¸»åŠ›èµ„é‡‘: >= threshold
        mask_main = df['æˆäº¤é¢(å…ƒ)'] >= threshold
        # æ•£æˆ·èµ„é‡‘: < threshold
        mask_retail = ~mask_main
        
        main_orders = df[mask_main]
        retail_orders = df[mask_retail]
        
        # 2. åˆ†ç±»æ±‡æ€» (è®¡ç®—æµå…¥æµå‡º)
        def calc_net(sub_df):
            # ä¸»åŠ¨ä¹°å…¥
            inflow = sub_df[sub_df['æ€§è´¨'].astype(str).str.contains('ä¹°')]['æˆäº¤é¢(å…ƒ)'].sum()
            # ä¸»åŠ¨å–å‡º
            outflow = sub_df[sub_df['æ€§è´¨'].astype(str).str.contains('å–')]['æˆäº¤é¢(å…ƒ)'].sum()
            net = inflow - outflow
            return float(inflow), float(outflow), float(net)
        
        main_in, main_out, main_net = calc_net(main_orders)
        retail_in, retail_out, retail_net = calc_net(retail_orders)

        return {
            "total_turnover": float(df['æˆäº¤é¢(å…ƒ)'].sum()),
            
            # ä¸»åŠ›èµ„é‡‘
            "large_order_net_inflow": main_net,
            "large_buy_amount": main_in,
            "large_sell_amount": main_out,
            "large_order_count": len(main_orders),
            
            # æ•£æˆ·èµ„é‡‘
            "retail_net_inflow": retail_net,
            "retail_buy_amount": retail_in,
            "retail_sell_amount": retail_out,
            "retail_order_count": len(retail_orders),
            
            # ç»Ÿè®¡
            "large_order_ratio": len(main_orders) / len(df) * 100 if len(df) > 0 else 0,
            "flow_quality": {
                "direction_source": meta.get("direction_source", "unknown"),
                "data_granularity": granularity,
                "large_order_threshold": float(threshold),
                "large_order_threshold_note": threshold_note,
            },
        }
    
    def get_algorithm_description(self) -> str:
        """è·å–ç®—æ³•è¯´æ˜"""
        t_val = self.large_order_threshold / 10000
        return f"""
### èµ„é‡‘æµå‘ç®—æ³• (Level-2 å¢å¼ºç‰ˆ)

#### ğŸ“Š èµ„é‡‘åˆ’åˆ†æ ‡å‡†
æ ¹æ®å•ç¬”æˆäº¤é‡‘é¢è¿›è¡Œåˆ’åˆ†ï¼š
- **ä¸»åŠ›èµ„é‡‘**: å•ç¬”æˆäº¤é¢ â‰¥ **{t_val:.0f}ä¸‡å…ƒ**
- **æ•£æˆ·èµ„é‡‘**: å•ç¬”æˆäº¤é¢ < **{t_val:.0f}ä¸‡å…ƒ**

#### ğŸ§® è®¡ç®—å…¬å¼
1. **ä¸»åŠ›å‡€æµå…¥** = ä¸»åŠ›ä¸»åŠ¨ä¹°å…¥é¢ - ä¸»åŠ›ä¸»åŠ¨å–å‡ºé¢
2. **æ•£æˆ·å‡€æµå…¥** = æ•£æˆ·ä¸»åŠ¨ä¹°å…¥é¢ - æ•£æˆ·ä¸»åŠ¨å–å‡ºé¢

#### ğŸ“ è¯´æ˜
- **æ•°æ®æº**: é€ç¬”æˆäº¤æ•°æ® (Tick Data)
- **ä¹°å–åˆ¤å®š**: æ ¹æ®æ¯ä¸€ç¬”äº¤æ˜“çš„ä¸»åŠ¨æ€§æ–¹å‘ï¼ˆä¸»åŠ¨ä¹°/ä¸»åŠ¨å–ï¼‰ç»Ÿè®¡
- è¿™æ˜¯ä¸šå†…é€šç”¨çš„èµ„é‡‘æµå‘è®¡ç®—é€»è¾‘ï¼Œèƒ½è¾ƒå¥½åœ°åæ˜ å¤§èµ„é‡‘çš„è¿›å‡ºæ„æ„¿ã€‚
"""
