"""
ä¸ªè‚¡åˆ†æé¡µé¢æ¨¡å—
"""
import logging
import streamlit as st
import pandas as pd
from datetime import datetime
from typing import Optional
import os
import json
import numpy as np

# å¯¼å…¥åˆ†æç»„ä»¶
from stock_analysis.data.providers.akshare_provider import AkShareProvider
from stock_analysis.data.providers.tushare_provider import TushareProvider
from stock_analysis.data.cleaner import DataCleaner, get_quality_summary
from stock_analysis.analysis.flows import FlowAnalyzer
from stock_analysis.analysis.timeseries import TimeSeriesAnalyzer
from stock_analysis.analysis.indicators import IndicatorCalculator
from stock_analysis.analysis.anomaly import AnomalyDetector
from stock_analysis.analysis.order_strength import OrderStrengthAnalyzer
from stock_analysis.analysis.tick_cleaner import TickDataCleaner
from stock_analysis.analysis.tick_flow import TickFlowAnalyzer
from stock_analysis.analysis.tick_aggregator import TickAggregator
from stock_analysis.analysis.tick_anomaly import TickAnomalyDetector
from stock_analysis.analysis.ai_client import get_deepseek_key, call_deepseek
from stock_analysis.visualization.charts import ChartGenerator
from stock_analysis.core.help_text import get_indicator_help, get_all_help_topics
from stock_analysis.core.cache_manager import CacheManager, DataImporter
from stock_analysis.core.config import settings
from stock_analysis.core.storage import StorageManager
from stock_analysis.data.stock_list import get_stock_provider

logger = logging.getLogger(__name__)

def show_analysis_page():
    st.header("ğŸ“ˆ ä¸ªè‚¡èµ„é‡‘æµå‘åˆ†æ")
    
    # åˆå§‹åŒ– Session State
    if 'stock_code' not in st.session_state:
        st.session_state.stock_code = "300661"
    
    # ä¾§è¾¹æ è¾…åŠ©åŠŸèƒ½
    with st.sidebar:
        st.subheader("ğŸ” è‚¡ç¥¨æœç´¢")
        stock_provider = get_stock_provider()
        search_query = st.text_input("æœç´¢ (ä»£ç /åç§°/æ‹¼éŸ³)", placeholder="å¦‚: 300661 æˆ– maotai")
        if search_query:
            results = stock_provider.search(search_query)
            if not results.empty:
                st.dataframe(results[['ä»£ç ', 'åç§°']], hide_index=True)
                # å¿«æ·é€‰æ‹©
                selected_code = st.selectbox("é€‰æ‹©è‚¡ç¥¨", results['ä»£ç '] + " | " + results['åç§°'])
                if selected_code:
                    code = selected_code.split(" | ")[0]
                    if st.button("åˆ†ææ­¤è‚¡ç¥¨"):
                        st.session_state.stock_code = code
                        st.rerun()
            else:
                st.caption("æœªæ‰¾åˆ°åŒ¹é…è‚¡ç¥¨")
    
    # ä¸»ç•Œé¢è¾“å…¥åŒº
    col_input1, col_input2, col_input3 = st.columns([2, 2, 3])
    
    with col_input1:
        stock_code = st.text_input("è‚¡ç¥¨ä»£ç ", value=st.session_state.stock_code, key="_input_code", help="è¾“å…¥6ä½è‚¡ç¥¨ä»£ç ")
        # æ›´æ–° session state
        st.session_state.stock_code = stock_code
        
    with col_input2:
        analysis_date = st.date_input("åˆ†ææ—¥æœŸ", value=datetime.now())
        if (datetime.now().date() - analysis_date).days > 7:
            st.caption("æç¤ºï¼šåˆ†é’Ÿçº§æ•°æ®æºé€šå¸¸ä»…ä¿ç•™æœ€è¿‘çº¦ 7 ä¸ªäº¤æ˜“æ—¥ï¼Œè¾ƒæ—©æ—¥æœŸå¯èƒ½è‡ªåŠ¨å›é€€æˆ–æ— æ•°æ®ã€‚")
        
    with col_input3:
        # æ·»åŠ /ç§»é™¤è‡ªé€‰è‚¡æŒ‰é’®
        storage = StorageManager()
        watchlist_codes = storage.get_watchlist_codes()
        is_in_watchlist = stock_code in watchlist_codes
        
        st.write("") # Spacer
        st.write("") # Spacer
        if is_in_watchlist:
            if st.button("ğŸ’” ç§»å‡ºè‡ªé€‰"):
                storage.remove_from_watchlist(stock_code)
                st.success("å·²ç§»é™¤")
                st.rerun()
        else:
            if st.button("â¤ï¸ åŠ å…¥è‡ªé€‰"):
                # è·å–åç§° (å¦‚æœèƒ½è·å–åˆ°)
                name = stock_code # é»˜è®¤
                try:
                    res = stock_provider.search(stock_code, limit=1)
                    if not res.empty:
                        name = res.iloc[0]['åç§°']
                except:
                    pass
                storage.add_to_watchlist(stock_code, name)
                st.success(f"å·²åŠ å…¥è‡ªé€‰: {name}")
                st.rerun()

    # æ•°æ®æºé€‰æ‹©
    with st.expander("âš™ï¸ é«˜çº§è®¾ç½® (æ•°æ®æº/å¯¼å…¥)", expanded=False):
        import_option = st.radio("æ•°æ®æ¨¡å¼", ["å®æ—¶è·å–", "å¯¼å…¥CSVæ–‡ä»¶"], horizontal=True)
        
        if import_option == "å¯¼å…¥CSVæ–‡ä»¶":
             uploaded_file = st.file_uploader("ä¸Šä¼ CSVæ–‡ä»¶", type=['csv'])
             if uploaded_file and st.button("å¯¼å…¥å¹¶åˆ†æ"):
                importer = DataImporter()
                df, success, msg = importer.import_from_csv(uploaded_file)
                if success:
                    st.success(msg)
                    process_imported_data(df, analysis_date)
                    st.rerun()
                else:
                    st.error(msg)
        else:
            # ä¿®æ”¹ï¼šAkShare è®¾ä¸ºé»˜è®¤æ¨è
            provider_choice = st.radio("APIæº", ["AkShare (æ¨è)", "Tushare Pro"], horizontal=True)
            tushare_token = ""
            if "Tushare" in provider_choice:
                tushare_token = st.text_input("Tushare Token", value=os.getenv("TUSHARE_TOKEN", ""), type="password")

    # å¼€å§‹åˆ†ææŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", use_container_width=True):
        date_str = analysis_date.strftime("%Y%m%d")
        
        if import_option == "å®æ—¶è·å–":
            with st.spinner(f"æ­£åœ¨è·å– {stock_code} åœ¨ {date_str} çš„æ•°æ®..."):
                df, actual_source, raw_df = fetch_data(stock_code, date_str, provider_choice, tushare_token)
                
                if df.empty:
                    requested_date = df.attrs.get('requested_date')
                    fallback_date = df.attrs.get('fallback_date')
                    if requested_date and fallback_date:
                        st.error(f"æ‰€é€‰æ—¥æœŸ {requested_date} æ— åˆ†é’Ÿæ•°æ®ï¼Œå›é€€åˆ° {fallback_date} ä»æœªè·å–åˆ°ã€‚")
                        st.caption("å»ºè®®ï¼šæ›´æ¢ä¸ºè¿‘æœŸäº¤æ˜“æ—¥æˆ–åˆ‡æ¢æ•°æ®æºã€‚")
                    elif requested_date:
                        st.error(f"æ‰€é€‰æ—¥æœŸ {requested_date} æš‚æ— åˆ†é’Ÿæ•°æ®ï¼Œè¯·æ›´æ¢æ—¥æœŸæˆ–ç¨åé‡è¯•ã€‚")
                    else:
                        st.error("æœªèƒ½è·å–æ•°æ®ï¼Œè¯·æ£€æŸ¥è‚¡ç¥¨ä»£ç æˆ–ç¨åé‡è¯•ã€‚")
                else:
                    requested_date = df.attrs.get('requested_date')
                    actual_date = df.attrs.get('actual_date')
                    if requested_date and actual_date and requested_date != actual_date:
                        st.info(f"æ‰€é€‰æ—¥æœŸæ— äº¤æ˜“æ•°æ®ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢åˆ°æœ€è¿‘äº¤æ˜“æ—¥ {actual_date}ã€‚")
                    process_and_display(df, stock_code, analysis_date, actual_source, raw_df)
    
    # æ˜¾ç¤ºå·²å­˜åœ¨çš„ç»“æœ (å¦‚æœæœ‰)
    if 'df' in st.session_state and st.session_state.df is not None:
        display_results(st.session_state.get('last_stock_code', stock_code), analysis_date)

# --- è¾…åŠ©å‡½æ•° ---

def _convert_tick_to_minute(df_tick: pd.DataFrame, analysis_date) -> tuple[pd.DataFrame, list]:
    analysis_day = analysis_date.date() if hasattr(analysis_date, "date") else analysis_date
    if analysis_day is None:
        analysis_day = datetime.now().date()

    cleaner = TickDataCleaner()
    clean_df, quality_flags, _, _ = cleaner.clean(df_tick, analysis_day)
    if clean_df.empty:
        return pd.DataFrame(), ["tick_clean_empty"]

    tick_df = clean_df.copy()
    tick_df["åˆ†é’Ÿ"] = tick_df["æ—¶é—´"].dt.floor("min")
    grouped = tick_df.groupby("åˆ†é’Ÿ", sort=True)

    minute_df = grouped["æˆäº¤ä»·æ ¼"].agg(["first", "last", "max", "min"]).rename(
        columns={"first": "å¼€ç›˜", "last": "æ”¶ç›˜", "max": "æœ€é«˜", "min": "æœ€ä½"}
    )
    minute_df["æˆäº¤é‡"] = grouped["æˆäº¤é‡"].sum()
    minute_df["æˆäº¤é¢"] = grouped["æˆäº¤é¢(å…ƒ)"].sum()
    minute_df = minute_df.reset_index().rename(columns={"åˆ†é’Ÿ": "æ—¶é—´"})
    minute_df["æˆäº¤é¢(å…ƒ)"] = minute_df["æˆäº¤é¢"]

    minute_df.attrs["raw_tick"] = df_tick
    minute_df.attrs["source_granularity"] = "tick_import"
    minute_df.attrs["imported_tick"] = True
    minute_df.attrs["analysis_date"] = analysis_day
    return minute_df, quality_flags


def process_imported_data(df, analysis_date=None):
    data_type = df.attrs.get("data_type", "minute") if df is not None else "minute"
    if data_type == "tick":
        minute_df, tick_flags = _convert_tick_to_minute(df, analysis_date)
        if minute_df.empty:
            st.error("Tick æ•°æ®æ¸…æ´—åä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆåˆ†é’Ÿæ•°æ®ã€‚")
            return

        cleaner = DataCleaner()
        df_clean, quality_report = cleaner.clean(minute_df)
        indicator_calc = IndicatorCalculator()
        df_with_indicators = indicator_calc.calculate_all(df_clean)

        st.session_state.df = df_with_indicators
        st.session_state.actual_source = "CSVå¯¼å…¥(Tick)"
        st.session_state.raw_df = minute_df.attrs.get("raw_tick", df)
        st.session_state.tick_context = _build_tick_context(
            st.session_state.raw_df, analysis_date, allow_imported=True
        )
        st.session_state.quality_report = quality_report
        st.session_state.all_analysis = perform_all_analysis(df_with_indicators)
        st.session_state.last_stock_code = "å¯¼å…¥æ•°æ®"
        if tick_flags:
            st.session_state.tick_import_flags = tick_flags
        return

    cleaner = DataCleaner()
    df_clean, quality_report = cleaner.clean(df)
    indicator_calc = IndicatorCalculator()
    df_with_indicators = indicator_calc.calculate_all(df_clean)

    st.session_state.df = df_with_indicators
    st.session_state.actual_source = "CSVå¯¼å…¥"
    st.session_state.raw_df = None
    st.session_state.tick_context = None
    st.session_state.quality_report = quality_report
    st.session_state.all_analysis = perform_all_analysis(df_with_indicators)
    st.session_state.last_stock_code = "å¯¼å…¥æ•°æ®"

def process_and_display(df, stock_code, analysis_date, actual_source, raw_df=None):
    cleaner = DataCleaner()
    df_clean, quality_report = cleaner.clean(df)
    indicator_calc = IndicatorCalculator()
    df_with_indicators = indicator_calc.calculate_all(df_clean)
    
    st.session_state.df = df_with_indicators
    st.session_state.actual_source = actual_source
    st.session_state.raw_df = raw_df
    st.session_state.tick_context = _build_tick_context(raw_df, analysis_date)
    st.session_state.quality_report = quality_report
    st.session_state.all_analysis = perform_all_analysis(df_with_indicators)
    st.session_state.last_stock_code = stock_code

def fetch_data(stock_code, date_str, provider_choice, tushare_token):
    actual_source = None
    df = pd.DataFrame()
    raw_df = None
    
    # æ ¹æ®é€‰æ‹©çš„é€»è¾‘
    use_tushare = "Tushare" in provider_choice
    
    if use_tushare:
        if not tushare_token:
            st.error("âŒ è¯·å…ˆè¾“å…¥ Tushare Token")
            return df, actual_source, raw_df
        try:
            os.environ["TUSHARE_TOKEN"] = tushare_token
            settings.TUSHARE_TOKEN = tushare_token
            provider = TushareProvider()
            df = provider.get_tick_data(stock_code, date_str=date_str)
            if not df.empty:
                actual_source = "Tushare Pro"
            else:
                raise ValueError("Empty data")
        except:
            st.warning("åˆ‡æ¢åˆ° AkShare...")
            provider = AkShareProvider()
            df = provider.get_tick_data(stock_code, date_str=date_str)
            actual_source = "AkShare (Fallback)"
    else:
        # AkShare ä¼˜å…ˆ
        provider = AkShareProvider()
        df = provider.get_tick_data(stock_code, date_str=date_str)
        actual_source = "AkShare"
    
    raw_df = df.attrs.get('raw_tick')
    return df, actual_source, raw_df

def perform_all_analysis(df):
    results = {}
    results['flows'] = FlowAnalyzer().calculate_flows(df)
    results['timeseries'] = TimeSeriesAnalyzer().analyze(df)
    results['indicators'] = IndicatorCalculator().get_summary(df)
    results['anomalies'] = AnomalyDetector().detect_all(df)
    
    sa = OrderStrengthAnalyzer()
    results['strength'] = sa.analyze(df)
    results['strength_timeseries'] = sa.get_minutely_strength(df)
    return results

def display_results(stock_code, analysis_date):
    df = st.session_state.df
    source = st.session_state.actual_source
    quality = st.session_state.quality_report
    analysis = st.session_state.all_analysis
    tick_context = st.session_state.get('tick_context')
    show_auction = False

    def _get_stock_name(code):
        if 'stock_name_cache' not in st.session_state:
            st.session_state.stock_name_cache = {}
        cache = st.session_state.stock_name_cache
        if code in cache:
            return cache[code]
        provider = get_stock_provider()
        name = code
        try:
            res = provider.search(code, limit=1)
            if not res.empty:
                name = res.iloc[0]['åç§°']
        except Exception:
            pass
        cache[code] = name
        return name
    
    # é¡¶éƒ¨çŠ¶æ€æ 
    current_time = datetime.now().strftime("%H:%M:%S")
    actual_date = df.attrs.get('actual_date') or analysis_date.strftime("%Y%m%d")
    requested_date = df.attrs.get('requested_date')
    actual_date_fmt = f"{actual_date[:4]}-{actual_date[4:6]}-{actual_date[6:]}"
    name = _get_stock_name(stock_code)

    date_note = f"åˆ†ææ—¥æœŸ: {actual_date_fmt}"
    if requested_date and requested_date != actual_date:
        requested_fmt = f"{requested_date[:4]}-{requested_date[4:6]}-{requested_date[6:]}"
        date_note = f"åˆ†ææ—¥æœŸ: {actual_date_fmt} (æ‰€é€‰: {requested_fmt})"

    source_note = source
    if tick_context:
        source_note = f"{source} + Tick"
    st.caption(
        f"æœ€åæ›´æ–°: {current_time} | {date_note} | åˆ†æå¯¹è±¡: {stock_code} {name} | æ•°æ®æº: {source_note} | è´¨é‡: {quality['quality_score']:.0f}/100"
    )

    if tick_context and tick_context.get("auction_df") is not None:
        if not tick_context["auction_df"].empty:
            show_auction = st.toggle(
                "æ˜¾ç¤ºé›†åˆç«ä»·",
                value=False,
                help="é»˜è®¤ä¸çº³å…¥ä¸»å›¾è¡¨ï¼Œå¼€å¯åä¼šåœ¨èµ„é‡‘æµå›¾ä¸­æ˜¾ç¤ºé›†åˆç«ä»·æ ‡è®°ã€‚",
            )

    tick_window_1m = tick_context.get("window_1m") if tick_context else None
    tick_window_5m = tick_context.get("window_5m") if tick_context else None
    tick_ofi_display = tick_context.get("ofi_display_df") if tick_context else None
    tick_clean_df = tick_context.get("clean_df") if tick_context else None
    combined_df = None

    if show_auction and tick_context:
        auction_processed = tick_context.get("auction_processed_df")
        if (
            auction_processed is not None
            and not auction_processed.empty
            and tick_clean_df is not None
            and not tick_clean_df.empty
        ):
            combined_df = pd.concat([tick_clean_df, auction_processed], ignore_index=True)
            combined_df = combined_df.sort_values("æ—¶é—´")
            windows = TickAggregator().aggregate(combined_df, windows=[1, 5, 10])
            tick_window_1m = windows.get(1, tick_window_1m)
            tick_window_5m = windows.get(5, tick_window_5m)

            if tick_window_1m is not None and not tick_window_1m.empty:
                tick_window_1m["cum_net_inflow"] = tick_window_1m["net_inflow"].cumsum()
                tick_window_1m["cum_net_inflow_ema"] = tick_window_1m["cum_net_inflow"].ewm(
                    alpha=0.2, adjust=False
                ).mean()

            if tick_window_1m is not None and not tick_window_1m.empty and "ofi" in tick_window_1m.columns:
                tick_ofi_display = tick_window_1m[["æ—¶é—´", "ofi"]].copy()
                tick_ofi_display["ofi"] = tick_ofi_display["ofi"].ewm(
                    alpha=0.3, adjust=False
                ).mean()
    
    # ===== ç¬¬ä¸€è¡Œï¼šæ ¸å¿ƒæŒ‡æ ‡å¡ç‰‡ (æ¢å¤5åˆ—å¸ƒå±€) =====
    ts_data = analysis.get('timeseries', {})
    ind_data = analysis.get('indicators', {})
    
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        change_pct = ts_data.get('price_change_pct', 0)
        st.metric("æ¶¨è·Œå¹…", f"{change_pct:+.2f}%", delta=f"{ts_data.get('price_change', 0):+.2f}")
    
    with col2:
        st.metric("æˆäº¤é¢", f"Â¥{ts_data.get('turnover_total', 0)/1e8:.2f}äº¿")
    
    with col3:
        st.metric("æŒ¯å¹…", f"{ts_data.get('amplitude', 0):.2f}%")
    
    with col4:
        vwap = ind_data.get('vwap', 0)
        close = ts_data.get('close_price', 0)
        vs_vwap = ((close - vwap) / vwap * 100) if vwap > 0 else 0
        st.metric("ä»·æ ¼ vs VWAP", f"{vs_vwap:+.2f}%", delta="é«˜äºå‡ä»·" if vs_vwap > 0 else "ä½äºå‡ä»·")
    
    with col5:
        large_orders = analysis.get('anomalies', {}).get('summary', {}).get('large_order_count', 0)
        if tick_context and tick_context.get("flow_summary"):
            large_orders = tick_context["flow_summary"].get("large_order_count", large_orders)
        st.metric("å¤§å•æ•°é‡", f"{large_orders} ç¬”")
    
    st.markdown("---")

    # ===== ç¬¬äºŒè¡Œï¼šæ ¸å¿ƒèµ°åŠ¿ =====
    cg = ChartGenerator()
    st.subheader("ğŸ“ˆ åˆ†æ—¶èµ°åŠ¿ + æˆäº¤é‡")
    st.plotly_chart(cg.create_candlestick_chart(df, stock_code), use_container_width=True)

    flows = analysis.get('flows', {})
    if tick_context and tick_context.get("flow_summary"):
        flows = tick_context["flow_summary"]
    total_net = flows.get('large_order_net_inflow', 0) + flows.get('retail_net_inflow', 0)
    col_f1, col_f2, col_f3 = st.columns(3)
    with col_f1:
        st.metric("ä¸»åŠ›å‡€æµå…¥", f"Â¥{flows.get('large_order_net_inflow', 0)/1e8:.2f}äº¿")
    with col_f2:
        st.metric("æ•£æˆ·å‡€æµå…¥", f"Â¥{flows.get('retail_net_inflow', 0)/1e8:.2f}äº¿")
    with col_f3:
        st.metric("æ€»å‡€æµå…¥", f"Â¥{total_net/1e8:.2f}äº¿")

    st.markdown("---")

    # ===== èµ„é‡‘æµå‘å…¨æ™¯ =====
    st.subheader("ğŸ’° èµ„é‡‘æµå‘å…¨æ™¯ç›‘æ§")
    df_chart = df.copy()

    if (
        tick_window_1m is not None
        and not tick_window_1m.empty
        and {"æ—¶é—´", "net_inflow", "turnover"}.issubset(tick_window_1m.columns)
    ):
        time_col = tick_window_1m["æ—¶é—´"]
        if isinstance(time_col, pd.DataFrame):
            time_col = time_col.iloc[:, 0]
        net_inflow_col = tick_window_1m["net_inflow"]
        if isinstance(net_inflow_col, pd.DataFrame):
            net_inflow_col = net_inflow_col.iloc[:, 0]
        turnover_col = tick_window_1m["turnover"]
        if isinstance(turnover_col, pd.DataFrame):
            turnover_col = turnover_col.iloc[:, 0]
        df_chart = pd.DataFrame(
            {
                "æ—¶é—´": time_col.values,
                "å‡€æµå…¥é¢": net_inflow_col.values,
                "æˆäº¤é¢(å…ƒ)": turnover_col.values,
            }
        )
        df_chart["ç´¯è®¡å‡€æµå…¥"] = df_chart["å‡€æµå…¥é¢"].cumsum()
        if "cum_net_inflow_ema" in tick_window_1m.columns:
            ema_col = tick_window_1m["cum_net_inflow_ema"]
            if isinstance(ema_col, pd.DataFrame):
                ema_col = ema_col.iloc[:, 0]
            df_chart["ç´¯è®¡å‡€æµå…¥_ema"] = ema_col.values
    else:
        def calc_net(row):
            amt = row.get('æˆäº¤é¢(å…ƒ)', row.get('amount', 0))
            nature = str(row.get('æ€§è´¨', ''))
            if 'ä¹°' in nature:
                return amt
            elif 'å–' in nature:
                return -amt
            return 0

        df_chart['å‡€æµå…¥é¢'] = df_chart.apply(calc_net, axis=1)
        df_chart['ç´¯è®¡å‡€æµå…¥'] = df_chart['å‡€æµå…¥é¢'].cumsum()

    if show_auction and tick_context and tick_context.get("auction_time"):
        marker_value = 0.0
        if not df_chart.empty and "ç´¯è®¡å‡€æµå…¥" in df_chart.columns:
            marker_value = float(df_chart["ç´¯è®¡å‡€æµå…¥"].iloc[0])
        df_chart.attrs["auction_marker"] = {
            "time": tick_context.get("auction_time"),
            "value": marker_value,
            "label": "é›†åˆç«ä»·",
        }

    col_a1, col_a2 = st.columns(2)

    with col_a1:
        st.markdown("**ğŸ“ˆ å…¨å¤©ç´¯è®¡èµ„é‡‘æµæ›²çº¿**")
        try:
            cum_flow_fig = cg.create_cumulative_flow_chart(df_chart)
            st.plotly_chart(cum_flow_fig, use_container_width=True)
        except Exception as e:
            st.error(f"ç´¯è®¡èµ„é‡‘æµæ›²çº¿ç”Ÿæˆå¤±è´¥: {e}")

    with col_a2:
        st.markdown("**ğŸŒ¡ï¸ æ—¥å†…åˆ†æ—¶èµ„é‡‘æµçƒ­åŠ›**")
        try:
            heatmap_fig = cg.create_intraday_heatmap(df_chart, resample_minutes=10)
            st.plotly_chart(heatmap_fig, use_container_width=True)
        except Exception as e:
            st.error(f"çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")

    st.markdown("---")

    # ===== èµ„é‡‘æµå‘æ·±åº¦åˆ†æ =====
    st.subheader("ğŸ” èµ„é‡‘æµå‘æ·±åº¦åˆ†æ")
    col_l, col_r = st.columns(2)

    flow_data = analysis.get('flows', {})
    if tick_context and tick_context.get("flow_summary"):
        flow_data = tick_context["flow_summary"]

    flow_source_df = df
    if tick_clean_df is not None:
        flow_source_df = tick_clean_df
    if combined_df is not None and not combined_df.empty:
        flow_source_df = combined_df

    stacked_area_fig = cg.create_stacked_area_flow(flow_source_df, flow_data, resample_minutes=30)

    strength_df = analysis.get('strength_timeseries', pd.DataFrame())
    if (
        tick_window_5m is not None
        and not tick_window_5m.empty
        and {"æ—¶é—´", "buy_amount", "sell_amount"}.issubset(tick_window_5m.columns)
    ):
        strength_df = tick_window_5m[["æ—¶é—´", "buy_amount", "sell_amount"]].rename(
            columns={"buy_amount": "ä¹°ç›˜é¢", "sell_amount": "å–ç›˜é¢"}
        )
    strength_fig = cg.create_order_strength_chart(strength_df)

    with col_l:
        st.markdown("**ğŸ’¼ ä¸»åŠ›/æ•£æˆ·èµ„é‡‘æµæ„æˆ (30åˆ†é’Ÿ)**")
        st.plotly_chart(stacked_area_fig, use_container_width=True)

        st.info(f"""
        **ä¸»åŠ›å‡€æµå…¥**: Â¥{flows.get('large_order_net_inflow', 0):,.0f}  
        **æ•£æˆ·å‡€æµå…¥**: Â¥{flows.get('retail_net_inflow', 0):,.0f}
        """)

    with col_r:
        st.markdown("**âš–ï¸ ä¹°å–ç›˜åŠ›åº¦å¯¹æ¯”**")
        st.plotly_chart(strength_fig, use_container_width=True)

    st.markdown("---")

    # ===== Tick èŠ‚å¥ç›‘æ§ =====
    if tick_window_5m is not None and not tick_window_5m.empty:
        st.subheader("ğŸ“Š Tick èŠ‚å¥ç›‘æ§")
        col_t1, col_t2 = st.columns(2)

        with col_t1:
            st.markdown("**ğŸ“ è®¢å•æµå¤±è¡¡ (OFI)**")
            ofi_source = tick_ofi_display
            if ofi_source is None or ofi_source.empty:
                if "ofi" in tick_window_5m.columns:
                    ofi_source = tick_window_5m[["æ—¶é—´", "ofi"]]
            if ofi_source is not None and not ofi_source.empty:
                ofi_fig = cg.create_ofi_trend_chart(ofi_source)
                st.plotly_chart(ofi_fig, use_container_width=True)
            else:
                st.info("æš‚æ—  OFI æ•°æ®")

        with col_t2:
            st.markdown("**ğŸ“Œ æˆäº¤å¯†åº¦ä¸æ³¢åŠ¨**")
            density_df = tick_window_1m if tick_window_1m is not None and not tick_window_1m.empty else tick_window_5m
            if density_df is not None and not density_df.empty:
                density_fig = cg.create_trade_density_chart(density_df)
                st.plotly_chart(density_fig, use_container_width=True)
            else:
                st.info("æš‚æ— æˆäº¤å¯†åº¦æ•°æ®")

        st.markdown("---")

    # ===== å¼‚åŠ¨ä¸è¿½è¸ª =====
    st.subheader("ğŸ“‰ ä»·æ ¼å¼‚åŠ¨ä¸å¤§å•è¿½è¸ª")
    col_cum, col_orders = st.columns(2)

    with col_cum:
        st.subheader("ğŸ“‰ ç´¯è®¡æ¶¨è·Œå¹…")
        if 'ç´¯è®¡æ¶¨è·Œå¹…' in df.columns:
            cum_fig = cg.create_cumulative_change_chart(df)
            st.plotly_chart(cum_fig, use_container_width=True)

    with col_orders:
        st.subheader("ğŸ¯ å¤§å•è¿½è¸ª")
        anomalies = analysis.get('anomalies', {})
        large_orders_list = anomalies.get('large_orders', [])
        if tick_context and tick_context.get("large_orders_list"):
            large_orders_list = tick_context["large_orders_list"]

        if large_orders_list:
            scatter_fig = cg.create_large_orders_scatter(large_orders_list, df)
            st.plotly_chart(scatter_fig, use_container_width=True)
        else:
            st.info("ä»Šæ—¥æš‚æ— å¼‚å¸¸å¤§å•")

    st.markdown("---")

    # ===== AI å›¾è¡¨è§£è¯» =====
    st.subheader("ğŸ¤– å›¾è¡¨è§£è¯» (AI)")
    st.caption("ä»…è§£è¯»å½“å‰å›¾è¡¨ï¼Œç‹¬ç«‹äºâ€œAI æ™ºèƒ½æŠ•é¡¾â€çš„å¯¹è¯")

    if "chart_ai_history" not in st.session_state:
        st.session_state.chart_ai_history = []
    if "chart_ai_last" not in st.session_state:
        st.session_state.chart_ai_last = None

    api_key, api_key_name = get_deepseek_key()
    if not api_key:
        st.info("æœªæ£€æµ‹åˆ° DeepSeek API Keyï¼Œè¯·å…ˆåœ¨ .env ä¸­é…ç½®åä½¿ç”¨ã€‚")
    else:
        st.caption(f"å½“å‰ä½¿ç”¨ç¯å¢ƒå˜é‡: {api_key_name}")
        stock_name = _get_stock_name(stock_code)
        current_key = f"{stock_code}:{actual_date}"
        focus = st.radio(
            "è§£è¯»ä¾§é‡ç‚¹",
            ["æ€»ä½“ç»“è®º", "èµ„é‡‘æµå‘", "é£é™©æç¤º"],
            horizontal=True,
            help="åˆ‡æ¢ä¾§é‡ç‚¹åï¼Œéœ€è¦ç‚¹å‡»â€œç”Ÿæˆå›¾è¡¨è§£è¯»â€æ‰ä¼šæ›´æ–°ç»“æœã€‚"
        )
        style = st.radio(
            "è§£è¯»é£æ ¼",
            ["ç®€æ´", "ä¸“ä¸š"],
            horizontal=True,
            help="ç®€æ´=è¦ç‚¹çŸ­å¥ï¼›ä¸“ä¸š=åˆ†å°æ ‡é¢˜ã€‚"
        )
        with st.expander("ğŸ“Œ ä¼ é€’ç»™æ¨¡å‹çš„æ•°æ®é¢„è§ˆ", expanded=False):
            chart_context = _build_chart_context(df, analysis, tick_context)
            st.json(chart_context)

        col_ai1, col_ai2 = st.columns([1, 3])
        with col_ai1:
            gen_chart_btn = st.button("ç”Ÿæˆå›¾è¡¨è§£è¯»", type="primary")
        with col_ai2:
            st.caption("æç¤ºï¼šç”Ÿæˆä¼šè°ƒç”¨å¤–éƒ¨APIï¼Œé€Ÿåº¦å–å†³äºç½‘ç»œã€‚")

        if gen_chart_btn:
            chart_context = _build_chart_context(df, analysis, tick_context)
            system_prompt, user_prompt = _build_chart_prompts(
                chart_context=chart_context,
                focus=focus,
                style=style
            )
            with st.spinner("æ­£åœ¨ç”Ÿæˆå›¾è¡¨è§£è¯»..."):
                try:
                    response = call_deepseek(
                        api_key=api_key,
                        system_prompt=system_prompt,
                        user_prompt=user_prompt,
                        temperature=0.2,
                        max_tokens=600
                    )
                    entry = {
                        "ts": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "key": current_key,
                        "stock_code": stock_code,
                        "stock_name": stock_name,
                        "actual_date": actual_date,
                        "focus": focus,
                        "style": style,
                        "response": response,
                        "context": chart_context,
                    }
                    st.session_state.chart_ai_history.append(entry)
                    st.session_state.chart_ai_last = entry
                except Exception as exc:
                    st.error(f"è¯·æ±‚å¤±è´¥: {exc}")

        if st.session_state.chart_ai_last and st.session_state.chart_ai_last.get("key") == current_key:
            st.markdown("### âœ… æœ€æ–°å›¾è¡¨è§£è¯»")
            st.caption(
                f"{st.session_state.chart_ai_last['ts']} | "
                f"{st.session_state.chart_ai_last['stock_code']} {st.session_state.chart_ai_last['stock_name']} | "
                f"{st.session_state.chart_ai_last['focus']} | "
                f"{st.session_state.chart_ai_last['style']}"
            )
            st.write(st.session_state.chart_ai_last["response"])
        else:
            st.info("å½“å‰è‚¡ç¥¨æš‚æ— å›¾è¡¨è§£è¯»ï¼Œè¯·ç‚¹å‡»â€œç”Ÿæˆå›¾è¡¨è§£è¯»â€ã€‚")

        if st.session_state.chart_ai_history:
            show_all = st.toggle("æ˜¾ç¤ºå…¨éƒ¨å†å²", value=False, help="é»˜è®¤åªå±•ç¤ºå½“å‰è‚¡ç¥¨ä¸æ—¥æœŸã€‚")
            history_items = st.session_state.chart_ai_history
            if not show_all:
                history_items = [item for item in history_items if item.get("key") == current_key]
            with st.expander("ğŸ—‚ï¸ å†å²å›¾è¡¨è§£è¯»", expanded=False):
                for item in reversed(history_items[-5:]):
                    st.markdown(
                        f"**{item['ts']} | {item['stock_code']} {item['stock_name']} | "
                        f"{item['focus']} | {item['style']}**"
                    )
                    st.write(item["response"])

    # ä¿å­˜åŠŸèƒ½
    st.subheader("ğŸ’¾ ä¿å­˜æ•°æ®")
    date_str = analysis_date.strftime("%Y%m%d")
    raw_df = st.session_state.get('raw_df')
    export_df = df
    file_suffix = "minute"
    if raw_df is not None and not raw_df.empty:
        use_tick = st.toggle("ä¸‹è½½ Tick æ•°æ®", value=True, help="ä»…å½“æ—¥å®æ—¶è·å–å¯ç”¨")
        if use_tick:
            export_df = raw_df
            file_suffix = "tick"
    csv = export_df.to_csv(index=False).encode('utf-8-sig')
    st.download_button("ä¸‹è½½ CSV", csv, f"{stock_code}_{date_str}_{file_suffix}.csv", "text/csv")


def _build_chart_context(df: pd.DataFrame, analysis: dict, tick_context: Optional[dict] = None) -> dict:
    timeseries = analysis.get('timeseries', {})
    flows = analysis.get('flows', {})
    if tick_context and tick_context.get("flow_summary"):
        flows = tick_context["flow_summary"]
    indicators = analysis.get('indicators', {})
    anomalies = analysis.get('anomalies', {})

    df_chart = df.copy()
    if tick_context and tick_context.get("window_1m") is not None:
        window_1m = tick_context["window_1m"]
        if (
            not window_1m.empty
            and {"æ—¶é—´", "net_inflow", "turnover"}.issubset(window_1m.columns)
        ):
            time_col = window_1m["æ—¶é—´"]
            if isinstance(time_col, pd.DataFrame):
                time_col = time_col.iloc[:, 0]
            net_inflow_col = window_1m["net_inflow"]
            if isinstance(net_inflow_col, pd.DataFrame):
                net_inflow_col = net_inflow_col.iloc[:, 0]
            turnover_col = window_1m["turnover"]
            if isinstance(turnover_col, pd.DataFrame):
                turnover_col = turnover_col.iloc[:, 0]
            df_chart = pd.DataFrame(
                {
                    "æ—¶é—´": time_col.values,
                    "å‡€æµå…¥é¢": net_inflow_col.values,
                    "æˆäº¤é¢(å…ƒ)": turnover_col.values,
                }
            )

    if 'æˆäº¤é¢(å…ƒ)' not in df_chart.columns:
        if 'æˆäº¤é¢' in df_chart.columns:
            df_chart['æˆäº¤é¢(å…ƒ)'] = df_chart['æˆäº¤é¢']
        elif 'amount' in df_chart.columns:
            df_chart['æˆäº¤é¢(å…ƒ)'] = df_chart['amount']

    def calc_net(row):
        amt = row.get('æˆäº¤é¢(å…ƒ)', 0)
        nature = str(row.get('æ€§è´¨', ''))
        if 'ä¹°' in nature:
            return amt
        if 'å–' in nature:
            return -amt
        return 0

    if not df_chart.empty:
        if 'å‡€æµå…¥é¢' not in df_chart.columns:
            df_chart['å‡€æµå…¥é¢'] = df_chart.apply(calc_net, axis=1)
        df_chart['ç´¯è®¡å‡€æµå…¥'] = df_chart['å‡€æµå…¥é¢'].cumsum()
        cum_flow_last = float(df_chart['ç´¯è®¡å‡€æµå…¥'].iloc[-1])
    else:
        cum_flow_last = 0.0

    total_net = flows.get('large_order_net_inflow', 0) + flows.get('retail_net_inflow', 0)

    context = {
        "charts": [
            "åˆ†æ—¶Kçº¿+æˆäº¤é‡",
            "ç´¯è®¡èµ„é‡‘æµæ›²çº¿",
            "èµ„é‡‘æµçƒ­åŠ›å›¾",
            "ä¸»åŠ›/æ•£æˆ·èµ„é‡‘æµæ„æˆ",
            "ä¹°å–ç›˜åŠ›åº¦å¯¹æ¯”",
            "ç´¯è®¡æ¶¨è·Œå¹…",
            "å¤§å•è¿½è¸ª",
        ],
        "price": {
            "open": timeseries.get("open_price"),
            "close": timeseries.get("close_price"),
            "high": timeseries.get("high_price"),
            "low": timeseries.get("low_price"),
            "change_pct": timeseries.get("price_change_pct"),
            "amplitude": timeseries.get("amplitude"),
        },
        "flow": {
            "large_net": flows.get("large_order_net_inflow"),
            "retail_net": flows.get("retail_net_inflow"),
            "total_net": total_net,
            "large_ratio": flows.get("large_order_ratio"),
            "cum_flow_last": cum_flow_last,
            "quality": flows.get("flow_quality", {}),
        },
        "indicators": {
            "vwap": indicators.get("vwap"),
            "ma5": indicators.get("ma5"),
            "ma10": indicators.get("ma10"),
            "price_vs_vwap": indicators.get("price_vs_vwap"),
        },
        "anomalies": {
            "large_order_count": anomalies.get("summary", {}).get("large_order_count", 0),
            "price_spike_count": anomalies.get("summary", {}).get("price_spike_count", 0),
            "volume_surge_count": anomalies.get("summary", {}).get("volume_surge_count", 0),
        },
    }

    if tick_context and tick_context.get("tick_ai_summary"):
        context["tick_summary"] = tick_context["tick_ai_summary"]

    return context


def _build_tick_context(raw_df: pd.DataFrame, analysis_date, allow_imported: bool = False) -> Optional[dict]:
    if raw_df is None or raw_df.empty:
        return None

    analysis_day = analysis_date.date() if hasattr(analysis_date, "date") else analysis_date
    allow_tick = allow_imported or bool(getattr(raw_df, "attrs", {}).get("imported_tick"))
    if not allow_tick and analysis_day != datetime.now().date():
        return None

    # ===== è¯Šæ–­å¿«ç…§ 1: raw_df åŸå§‹çŠ¶æ€ =====
    logger.info("=" * 60)
    logger.info("ğŸ” Tick è¯Šæ–­å¿«ç…§ - é˜¶æ®µ 1: raw_df åŸå§‹çŠ¶æ€")
    logger.info(f"raw_df.shape: {raw_df.shape}")
    logger.info(f"raw_df.columns: {list(raw_df.columns)}")
    
    nature_col = raw_df.get("æ€§è´¨")
    if nature_col is not None:
        nature_dist = nature_col.value_counts().to_dict()
        nature_notnull_ratio = nature_col.notna().sum() / len(raw_df) if len(raw_df) > 0 else 0
        logger.info(f"æ€§è´¨_åˆ†å¸ƒ: {nature_dist}")
        logger.info(f"æ€§è´¨_éç©ºæ¯”ä¾‹: {nature_notnull_ratio:.2%}")
        logger.info(f"æ€§è´¨_æ ·æœ¬å‰5è¡Œ: {nature_col.head().tolist()}")
    else:
        logger.warning("âŒ raw_df ç¼ºå°‘ 'æ€§è´¨' åˆ—")
    
    if "æˆäº¤é¢" in raw_df.columns or "æˆäº¤é¢(å…ƒ)" in raw_df.columns:
        amount_col = "æˆäº¤é¢(å…ƒ)" if "æˆäº¤é¢(å…ƒ)" in raw_df.columns else "æˆäº¤é¢"
        amount_nonzero = (pd.to_numeric(raw_df[amount_col], errors='coerce') != 0).sum()
        logger.info(f"{amount_col}_éé›¶æ•°é‡: {amount_nonzero}/{len(raw_df)}")
    logger.info("=" * 60)

    cleaner = TickDataCleaner()
    clean_df, quality_flags, auction_df, inferred_ratio = cleaner.clean(raw_df, analysis_day)
    if clean_df.empty:
        logger.error("âŒ clean_df ä¸ºç©ºï¼Œæ¸…æ´—å¤±è´¥")
        return None
    
    # ===== è¯Šæ–­å¿«ç…§ 2: clean_df æ¸…æ´—åçŠ¶æ€ =====
    logger.info("ğŸ” Tick è¯Šæ–­å¿«ç…§ - é˜¶æ®µ 2: clean_df æ¸…æ´—åçŠ¶æ€")
    logger.info(f"clean_df.shape: {clean_df.shape}")
    
    clean_nature_col = clean_df.get("æ€§è´¨")
    if clean_nature_col is not None:
        nature_dist_clean = clean_nature_col.value_counts().to_dict()
        nature_na_count = clean_nature_col.isna().sum()
        logger.info(f"æ€§è´¨_åˆ†å¸ƒ: {nature_dist_clean}")
        logger.info(f"æ€§è´¨_NAæ•°é‡: {nature_na_count}/{len(clean_df)}")
        logger.info(f"æ€§è´¨_æ ·æœ¬å‰5è¡Œ: {clean_nature_col.head().tolist()}")
    else:
        logger.warning("âŒ clean_df ç¼ºå°‘ 'æ€§è´¨' åˆ—")
    
    if "æˆäº¤é¢(å…ƒ)" in clean_df.columns:
        amount_nonzero_clean = (clean_df["æˆäº¤é¢(å…ƒ)"] != 0).sum()
        logger.info(f"æˆäº¤é¢(å…ƒ)_éé›¶æ•°é‡: {amount_nonzero_clean}/{len(clean_df)}")
        logger.info(f"æˆäº¤é¢(å…ƒ)_æ ·æœ¬å‰5è¡Œ: {clean_df['æˆäº¤é¢(å…ƒ)'].head().tolist()}")
    
    logger.info(f"quality_flags: {quality_flags}")
    logger.info(f"inferred_ratio: {inferred_ratio:.2%}")
    logger.info("=" * 60)

    flow_analyzer = TickFlowAnalyzer()
    flow_result = flow_analyzer.analyze(clean_df)
    processed_df = flow_result.get("processed_df", clean_df)
    quality_flags.extend(flow_result.get("quality_flags", []))
    
    # ===== è¯Šæ–­å¿«ç…§ 3: flow_result åˆ†æåçŠ¶æ€ =====
    logger.info("ğŸ” Tick è¯Šæ–­å¿«ç…§ - é˜¶æ®µ 3: flow_result åˆ†æåçŠ¶æ€")
    
    if "æ–¹å‘" in processed_df.columns:
        direction_dist = processed_df["æ–¹å‘"].value_counts().to_dict()
        logger.info(f"æ–¹å‘_åˆ†å¸ƒ: {direction_dist}")
        logger.info(f"æ–¹å‘_æ ·æœ¬å‰10è¡Œ: {processed_df['æ–¹å‘'].head(10).tolist()}")
    else:
        logger.warning("âŒ processed_df ç¼ºå°‘ 'æ–¹å‘' åˆ—")
    
    summary = flow_result.get("summary", {})
    logger.info(f"buy_amount: {summary.get('buy_amount', 0):,.2f}")
    logger.info(f"sell_amount: {summary.get('sell_amount', 0):,.2f}")
    logger.info(f"net_inflow: {summary.get('net_inflow', 0):,.2f}")
    logger.info(f"ofi: {summary.get('ofi', 0):.4f}")
    logger.info(f"trade_count: {summary.get('trade_count', 0)}")
    logger.info(f"buy_count: {summary.get('buy_count', 0)}, sell_count: {summary.get('sell_count', 0)}, neutral_count: {summary.get('neutral_count', 0)}")
    logger.info(f"flow_quality_flags: {flow_result.get('quality_flags', [])}")
    logger.info("=" * 60)

    auction_processed_df = pd.DataFrame()
    if auction_df is not None and not auction_df.empty:
        auction_flow = flow_analyzer.analyze(auction_df)
        auction_processed_df = auction_flow.get("processed_df", auction_df)

    aggregator = TickAggregator()
    windows = aggregator.aggregate(processed_df, windows=[1, 5, 10])
    window_1m = windows.get(1, pd.DataFrame())
    window_5m = windows.get(5, pd.DataFrame())
    window_10m = windows.get(10, pd.DataFrame())
    
    # ===== è¯Šæ–­å¿«ç…§ 4: aggregator èšåˆåçŠ¶æ€ =====
    logger.info("ğŸ” Tick è¯Šæ–­å¿«ç…§ - é˜¶æ®µ 4: aggregator èšåˆåçŠ¶æ€")
    logger.info(f"window_1m.shape: {window_1m.shape if not window_1m.empty else 'EMPTY'}")
    logger.info(f"window_5m.shape: {window_5m.shape if not window_5m.empty else 'EMPTY'}")
    
    if not window_5m.empty:
        logger.info(f"window_5m.columns: {list(window_5m.columns)}")
        display_cols = ["time_window", "buy_amount", "sell_amount", "net_inflow", "ofi", "turnover"]
        available_cols = [c for c in display_cols if c in window_5m.columns]
        logger.info(f"window_5m å‰5è¡Œå…³é”®å­—æ®µ:\n{window_5m[available_cols].head().to_string()}")
        
        if "buy_amount" in window_5m.columns:
            logger.info(f"buy_amount éé›¶æ•°é‡: {(window_5m['buy_amount'] != 0).sum()}/{len(window_5m)}")
        if "sell_amount" in window_5m.columns:
            logger.info(f"sell_amount éé›¶æ•°é‡: {(window_5m['sell_amount'] != 0).sum()}/{len(window_5m)}")
        if "ofi" in window_5m.columns:
            logger.info(f"ofi éé›¶æ•°é‡: {(window_5m['ofi'] != 0).sum()}/{len(window_5m)}")
    else:
        logger.error("âŒ window_5m ä¸ºç©º")
    
    logger.info("=" * 60)
    logger.info("âœ… Tick è¯Šæ–­å¿«ç…§å®Œæˆ")
    logger.info("=" * 60)

    ofi_display_df = pd.DataFrame()
    if not window_1m.empty and "ofi" in window_1m.columns:
        ofi_display_df = window_1m[["æ—¶é—´", "ofi"]].copy()
        ofi_display_df["ofi"] = ofi_display_df["ofi"].ewm(alpha=0.3, adjust=False).mean()

        if not window_5m.empty:
            ofi_ema_5m = (
                ofi_display_df.set_index("æ—¶é—´")["ofi"]
                .resample("5min")
                .last()
                .reset_index()
                .rename(columns={"ofi": "ofi_ema"})
            )
            window_5m = window_5m.merge(ofi_ema_5m, on="æ—¶é—´", how="left")

    if not window_1m.empty and "net_inflow" in window_1m.columns:
        window_1m["cum_net_inflow"] = window_1m["net_inflow"].cumsum()
        window_1m["cum_net_inflow_ema"] = window_1m["cum_net_inflow"].ewm(
            alpha=0.2, adjust=False
        ).mean()

    anomaly_detector = TickAnomalyDetector()
    anomaly_source = window_5m if not window_5m.empty else window_1m
    anomaly_result = anomaly_detector.detect(processed_df, anomaly_source)

    large_orders_df = flow_result.get("large_orders", pd.DataFrame())
    large_orders_list = []
    if large_orders_df is not None and not large_orders_df.empty:
        large_orders_df = large_orders_df.sort_values("æ—¶é—´")
        time_score = pd.Series(
            np.linspace(1.0, 0.3, len(large_orders_df)),
            index=large_orders_df.index,
        )
        large_orders_df["weighted_amount"] = large_orders_df["æˆäº¤é¢(å…ƒ)"] * time_score
        top_orders = large_orders_df.sort_values("weighted_amount", ascending=False).head(15)
        for _, row in top_orders.iterrows():
            large_orders_list.append(
                {
                    "time": row.get("æ—¶é—´"),
                    "amount": float(row.get("æˆäº¤é¢(å…ƒ)", 0)),
                    "price": float(row.get("æˆäº¤ä»·æ ¼", row.get("æ”¶ç›˜", 0))),
                    "type": row.get("æ€§è´¨", "ä¸­æ€§ç›˜"),
                    "ratio": float(row.get("ratio", 0)),
                }
            )

    summary = flow_result.get("summary", {})
    trade_count = summary.get("trade_count", 0) or 1
    flow_summary = {
        "total_turnover": summary.get("total_turnover", 0),
        "large_order_net_inflow": summary.get("large_order_net_inflow", 0),
        "retail_net_inflow": summary.get("retail_net_inflow", 0),
        "large_order_ratio": summary.get("large_order_count", 0) / trade_count * 100,
        "large_order_count": summary.get("large_order_count", 0),
        "large_buy_amount": summary.get("large_buy_amount", 0),
        "large_sell_amount": summary.get("large_sell_amount", 0),
        "retail_buy_amount": summary.get("retail_buy_amount", 0),
        "retail_sell_amount": summary.get("retail_sell_amount", 0),
        "flow_quality": {
            "direction_source": "tick",
            "data_granularity": "tick",
            "large_order_threshold": summary.get("large_order_threshold", 0),
            "large_order_threshold_early": summary.get("large_order_threshold_early", 0),
            "large_order_threshold_note": "per_minute_percentile_or_min",
        },
        "trade_count": summary.get("trade_count", 0),
        "buy_count": summary.get("buy_count", 0),
        "sell_count": summary.get("sell_count", 0),
        "neutral_count": summary.get("neutral_count", 0),
        "buy_amount": summary.get("buy_amount", 0),
        "sell_amount": summary.get("sell_amount", 0),
        "net_inflow": summary.get("net_inflow", 0),
        "buy_ratio": summary.get("buy_ratio", 0),
        "sell_ratio": summary.get("sell_ratio", 0),
        "ofi": summary.get("ofi", 0),
    }

    volume_unit = "shares" if "volume_unit_shares" in quality_flags else "unknown"
    auction_time = None
    if not auction_df.empty and "æ—¶é—´" in auction_df.columns:
        auction_time = auction_df["æ—¶é—´"].max()
    auction_summary = {
        "auction_volume": float(auction_df["æˆäº¤é‡"].sum()) if not auction_df.empty else 0.0,
        "auction_amount": float(auction_df["æˆäº¤é¢(å…ƒ)"].sum()) if not auction_df.empty else 0.0,
        "open_gap_percent": None,
        "open_gap_available": False,
        "auction_time": str(auction_time) if auction_time is not None else "",
    }

    base_window_df = window_5m if not window_5m.empty else window_1m
    tick_ai_summary = {}
    if base_window_df is not None and not base_window_df.empty:
        time_windows = base_window_df["time_window"].tolist()
        ofi_series = (
            base_window_df["ofi_ema"] if "ofi_ema" in base_window_df.columns else base_window_df["ofi"]
        )
        tick_ai_summary = {
            "core_20w": {
                "ofi_trend": ofi_series.tail(20).tolist(),
                "net_inflow_trend": base_window_df["net_inflow"].tail(20).tolist(),
                "large_order_counts": base_window_df.get("large_order_count", 0).tail(20).tolist()
                if "large_order_count" in base_window_df.columns
                else [],
            },
            "detail_40w": {
                "time_windows": time_windows[-40:],
                "buy_pressure": base_window_df["buy_amount"].tail(40).tolist()
                if "buy_amount" in base_window_df.columns
                else [],
                "sell_pressure": base_window_df["sell_amount"].tail(40).tolist()
                if "sell_amount" in base_window_df.columns
                else [],
                "price_volatility": base_window_df["range_pct"].tail(40).tolist()
                if "range_pct" in base_window_df.columns
                else [],
            },
            "extended_60w": {
                "available": True,
            },
            "metadata": {
                "summary_range": "40w",
                "inferred_ratio": round(inferred_ratio, 4),
                "volume_unit": volume_unit,
                "note": "All volume data is standardized to shares (1 hand = 100 shares).",
                "update_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            },
            "auction_summary": auction_summary,
        }

    return {
        "clean_df": processed_df,
        "flow_summary": flow_summary,
        "window_1m": window_1m,
        "window_5m": window_5m,
        "window_10m": window_10m,
        "large_orders_list": large_orders_list,
        "large_orders_top5": large_orders_list[:5],
        "ofi_display_df": ofi_display_df,
        "auction_df": auction_df,
        "auction_processed_df": auction_processed_df,
        "auction_summary": auction_summary,
        "auction_time": auction_time,
        "tick_ai_summary": tick_ai_summary,
        "volume_unit": volume_unit,
        "inferred_ratio": inferred_ratio,
        "burst_windows": anomaly_result.get("burst_windows", []),
        "anomaly_notes": anomaly_result.get("anomaly_notes", []),
        "quality_flags": quality_flags,
    }


def _build_chart_prompts(chart_context: dict, focus: str, style: str) -> tuple[str, str]:
    style_map = {
        "ç®€æ´": "4-6æ¡è¦ç‚¹ï¼Œå¥å­çŸ­",
        "ä¸“ä¸š": "åˆ†å°æ ‡é¢˜+è¦ç‚¹",
    }
    system_prompt = (
        "ä½ æ˜¯Aè‚¡æ—¥å†…å›¾è¡¨è§£è¯»åŠ©æ‰‹ï¼Œåªèƒ½å›´ç»•äº¤æ˜“ä¸é‡‘èè¯é¢˜å›ç­”ã€‚"
        "ä¸è¦ç»™å‡ºä¹°å–æŒ‡ä»¤ï¼Œåªè§£é‡Šå›¾è¡¨å«ä¹‰ä¸é£é™©ã€‚"
    )
    user_prompt = {
        "ä»»åŠ¡": "è§£è¯»å½“å‰é¡µé¢å›¾è¡¨ï¼Œä¸è¦å‘æ•£åˆ°å…¶ä»–ä¸»é¢˜",
        "è§£è¯»ä¾§é‡ç‚¹": focus,
        "è¾“å‡ºé£æ ¼": style_map.get(style, style),
        "æ•°æ®å¿«ç…§": chart_context,
        "è¾“å‡ºæ ¼å¼": [
            "æ€»ä½“ç»“è®º(1-2å¥)",
            "å›¾è¡¨è¦ç‚¹(é€æ¡)",
            "é£é™©/ä¸ç¡®å®šæ€§",
            "è§‚å¯Ÿæ¸…å•(è§¦å‘æ¡ä»¶)"
        ],
    }
    return system_prompt, json.dumps(user_prompt, ensure_ascii=False, indent=2, default=str)
