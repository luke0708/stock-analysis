"""
æœªæ¥åŠŸèƒ½å ä½é¡µ (Mockups)
å±•ç¤ºå³å°†æ¨å‡ºçš„åŠŸèƒ½é¢„è§ˆå›¾ï¼Œè®©ç”¨æˆ·æ›´æœ‰å®æ„Ÿ
"""
import streamlit as st
import pandas as pd
import numpy as np
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional

from stock_analysis.data.stock_list import get_stock_provider
from stock_analysis.analysis.ai_client import get_deepseek_key, call_deepseek
from stock_analysis.data.news_provider import StockNewsProvider


@st.cache_data(ttl=300)
def _load_stock_news(stock_code: str, limit: int) -> pd.DataFrame:
    return StockNewsProvider.get_stock_news(stock_code, limit=limit)


def _build_news_payload(news_df: Optional[pd.DataFrame], stock_name: str) -> Dict:
    if news_df is None or news_df.empty:
        return {
            "has_news": False,
            "source": "AkShare",
            "stock_name": stock_name,
            "items": [],
        }

    items = []
    for _, row in news_df.head(6).iterrows():
        items.append({
            "time": row.get("å‘å¸ƒæ—¶é—´", ""),
            "title": row.get("æ–°é—»æ ‡é¢˜", ""),
            "summary": row.get("æ–°é—»å†…å®¹", ""),
        })

    return {
        "has_news": True,
        "source": "AkShare",
        "stock_name": stock_name,
        "items": items,
        "latest_time": items[0]["time"] if items else None,
    }

def show_multi_stock_compare():
    st.header("âš–ï¸ å¤šè‚¡ç¥¨å¯¹æ¯”åˆ†æ (Coming Soon)")
    st.info("ğŸš§ æ­¤åŠŸèƒ½å°†åœ¨ v1.2 ç‰ˆæœ¬ä¸Šçº¿")
    
    col1, col2 = st.columns([1, 1])
    with col1:
        st.subheader("åŠŸèƒ½é¢„è§ˆ")
        st.markdown("""
        - **å¤šç»´å åŠ **: åŒæ—¶æŸ¥çœ‹æœ€å¤š 5 åªè‚¡ç¥¨çš„èµ°åŠ¿
        - **ç›¸å¯¹æ”¶ç›Š**: ä»¥æŸæ—¥ä¸ºåŸºå‡†æŸ¥çœ‹ç›¸å¯¹æ¶¨è·Œå¹…
        - **èµ„é‡‘æµå¯¹æ¯”**: æ¨ªå‘æ¯”è¾ƒè°çš„ä¸»åŠ›ä»‹å…¥æ›´æ·±
        """)
        
    with col2:
        # Mock chart
        dates = pd.date_range(end=pd.Timestamp.now(), periods=100)
        # Generate trend data (cumulative sum) + ensure no infinities
        np.random.seed(42)  # Fixed seed for stability
        data = pd.DataFrame(
            np.random.randn(100, 3).cumsum(0),
            index=dates,
            columns=['è´µå·èŒ…å° (Mock)', 'å®å¾·æ—¶ä»£ (Mock)', 'æ‹›å•†é“¶è¡Œ (Mock)']
        )
        # Add offset to avoid 0/negative if using log scale (though line_chart defaults to linear)
        data = data + 100 
        
        st.line_chart(data)

def show_backtesting():
    st.header("ğŸ§ª ç­–ç•¥å›æµ‹å®éªŒå®¤ (Coming Soon)")
    st.warning("ğŸš§ æ­¤åŠŸèƒ½å°†åœ¨ v1.3 ç‰ˆæœ¬ä¸Šçº¿")
    
    st.markdown("### é¢„è®¾ç­–ç•¥é…ç½®")
    c1, c2, c3 = st.columns(3)
    c1.selectbox("äº¤æ˜“ç­–ç•¥", ["åŒå‡çº¿äº¤å‰", "RSIè¶…ä¹°è¶…å–", "ç½‘æ ¼äº¤æ˜“"])
    c2.date_input("å›æµ‹å¼€å§‹", value=pd.to_datetime("2023-01-01"))
    c3.number_input("åˆå§‹èµ„é‡‘", value=100000)
    
    st.button("å¼€å§‹å›æµ‹ (æ¼”ç¤ºæŒ‰é’®)", disabled=True)
    
    st.markdown("### é¢„æœŸå›æµ‹æŠ¥å‘Š")
    st.write("ğŸ“ˆ å¹´åŒ–æ”¶ç›Šç‡: 15.2% | ğŸ“‰ æœ€å¤§å›æ’¤: -8.5% | ğŸ¯ èƒœç‡: 58%")
    
def show_global_markets():
    st.header("ğŸŒ å…¨çƒå¸‚åœºæ¦‚è§ˆ (Coming Soon)")
    st.success("ğŸš§ é•¿æœŸè§„åˆ’åŠŸèƒ½ (v2.2)")
    
    cols = st.columns(4)
    cols[0].metric("çº³æ–¯è¾¾å…‹", "14,890.30", "+1.2%")
    cols[1].metric("æ’ç”ŸæŒ‡æ•°", "16,500.00", "-0.5%")
    cols[2].metric("æ—¥ç»225", "35,000.00", "+0.8%")
    cols[3].metric("æ ‡æ™®500", "4,780.00", "+0.9%")
    
    st.caption("*ä»¥ä¸Šæ•°æ®ä»…ä¸ºé™æ€æ¼”ç¤º*")

def show_ai_analysis():
    st.header("ğŸ¤– AI æ™ºèƒ½æŠ•é¡¾")
    st.caption("ä¸“æ³¨äºAè‚¡èµ„é‡‘æµå‘ä¸æ—¥å†…äº¤æ˜“è§£è¯»ï¼Œè¾“å‡ºä¸ºç»“æ„åŒ–ç»“è®º")

    api_key, api_key_name = get_deepseek_key()
    if not api_key:
        st.warning("æœªæ£€æµ‹åˆ° DeepSeek API Keyï¼Œè¯·åœ¨ .env ä¸­é…ç½®åå†ä½¿ç”¨ã€‚")
        st.code("DEEPSEEK_API_KEY=ä½ çš„key", language="bash")
        return

    st.caption(f"å½“å‰ä½¿ç”¨ç¯å¢ƒå˜é‡: {api_key_name}")

    if "df" not in st.session_state or st.session_state.df is None:
        st.info("è¯·å…ˆåœ¨â€œä¸ªè‚¡èµ„é‡‘æµå‘â€é¡µé¢å®Œæˆä¸€æ¬¡åˆ†æï¼Œä»¥ä¾¿ç”Ÿæˆæ›´å‡†ç¡®çš„ AI è§£è¯»ã€‚")
        return

    if "ai_history" not in st.session_state:
        st.session_state.ai_history = []
    if "ai_last" not in st.session_state:
        st.session_state.ai_last = None
    if "ai_news_df" not in st.session_state:
        st.session_state.ai_news_df = None
    if "ai_news_stock" not in st.session_state:
        st.session_state.ai_news_stock = ""
    if "ai_news_limit" not in st.session_state:
        st.session_state.ai_news_limit = 0

    with st.expander("â„¹ï¸ è¾“å…¥æ•°æ®è¯´æ˜", expanded=False):
        st.write(
            "æ¨¡å‹è¾“å…¥æ¥è‡ªæœ€è¿‘ä¸€æ¬¡ä¸ªè‚¡åˆ†æç»“æœï¼ŒåŒ…å«ä»·æ ¼ã€èµ„é‡‘æµã€æŠ€æœ¯æŒ‡æ ‡ä¸å¼‚åŠ¨ç»Ÿè®¡ã€‚"
            "åŸå§‹æ•°æ®å·²ç»è¿‡ DataCleaner æ¸…æ´—ï¼ˆä¿®å¤ç¼ºå¤±å€¼/å¼‚å¸¸å€¼ã€æ ‡å‡†åŒ–ç±»å‹ï¼‰ã€‚"
            "æ¨¡å‹è¾“å‡ºé«˜åº¦ä¾èµ–è¿™äº›ç»“æ„åŒ–æ•°æ®ï¼Œå› æ­¤åˆ‡æ¢æ—¥æœŸæˆ–è‚¡ç¥¨ä¼šæ˜¾è‘—å½±å“è§£è¯»ã€‚"
        )

    st.markdown("### ğŸ¯ åˆ†æç›®æ ‡")
    focus = st.radio(
        "è¯·é€‰æ‹©åˆ†æä¾§é‡ç‚¹",
        ["èµ„é‡‘æµå‘è§£è¯»", "ç›˜ä¸­è¶‹åŠ¿ä¸èŠ‚å¥", "é£é™©ä¸å¼‚åŠ¨", "ä¸»åŠ›è¡Œä¸ºå¤ç›˜"],
        horizontal=True,
        help="åˆ‡æ¢ä¾§é‡ç‚¹ä¼šæ”¹å˜æç¤ºè¯ï¼Œä½†éœ€è¦ç‚¹å‡»â€œç”Ÿæˆè§£è¯»â€æ‰ä¼šæ›´æ–°ç»“æœã€‚"
    )

    style = st.radio(
        "è¾“å‡ºé£æ ¼",
        ["ç®€æ´", "ä¸“ä¸š", "äº¤æ˜“å‘˜é£æ ¼"],
        horizontal=True,
        help="ç®€æ´=è¦ç‚¹çŸ­å¥ï¼›ä¸“ä¸š=åˆ†å°æ ‡é¢˜ï¼›äº¤æ˜“å‘˜=æ›´å¼ºè°ƒç›˜ä¸­èŠ‚å¥ã€‚"
    )

    col1, col2 = st.columns(2)
    with col1:
        avoid_advice = st.checkbox(
            "é¿å…ä¹°å–æŒ‡ä»¤",
            value=True,
            help="ä»…åšåˆ†æï¼Œä¸ç›´æ¥ç»™å‡ºä¹°/å–æŒ‡ä»¤ã€‚"
        )
        only_data = st.checkbox(
            "åªåŸºäºç»™å®šæ•°æ®",
            value=True,
            help="åªä½¿ç”¨å½“å‰åˆ†æç»“æœï¼Œä¸å¼•å…¥å¤–éƒ¨ä¿¡æ¯ã€‚"
        )
    with col2:
        highlight_numbers = st.checkbox(
            "çªå‡ºå…³é”®æ•°å€¼",
            value=True,
            help="å¿…é¡»å¼•ç”¨å…³é”®æ•°æ®ä½œä¸ºè®ºæ®ã€‚"
        )
        add_watchlist = st.checkbox(
            "ç»™å‡ºè§‚å¯Ÿæ¸…å•",
            value=True,
            help="åˆ—å‡ºåç»­å…³æ³¨çš„è§¦å‘æ¡ä»¶æˆ–å…³é”®å˜é‡ã€‚"
        )

    temperature = st.slider(
        "è¾“å‡ºå¤šæ ·æ€§ (temperature)",
        min_value=0.0,
        max_value=0.8,
        value=0.2,
        step=0.1,
        help="æ•°å€¼è¶Šä½è¶Šç¨³å®šã€è¶Šæ¥è¿‘ç¡®å®šè¾“å‡ºï¼›æ•°å€¼è¶Šé«˜è¶Šå¤šæ ·åŒ–ã€‚"
    )

    user_question = st.text_area(
        "è¡¥å……é—®é¢˜ï¼ˆå¯é€‰ï¼‰",
        placeholder="ä¾‹å¦‚ï¼šä»Šå¤©ä¸»åŠ›å¸ç­¹æ˜¯å¦æ˜æ˜¾ï¼ŸçŸ­çº¿æœ‰å“ªäº›é£é™©ç‚¹ï¼Ÿ",
        help="è¡¥å……å…·ä½“é—®é¢˜ä¼šæ”¹å˜è§£è¯»é‡ç‚¹ï¼›è‹¥æ¶‰åŠæ–°é—»ï¼Œè¯·å¼€å¯ä¸‹æ–¹â€œåŒ…å«æœ€æ–°ç›¸å…³æ–°é—»â€ã€‚"
    )

    st.markdown("### ğŸ“° æ–°é—»è¡¥å……ï¼ˆå¯é€‰ï¼‰")
    include_news = st.checkbox(
        "åŒ…å«æœ€æ–°ç›¸å…³æ–°é—»",
        value=False,
        help="ä» AkShare æ‹‰å–ç›¸å…³æ–°é—»ï¼Œå¯èƒ½æœ‰å»¶è¿Ÿæˆ–ç¼ºå¤±ã€‚"
    )
    news_limit = st.slider(
        "æ–°é—»æ¡æ•°",
        min_value=3,
        max_value=12,
        value=6,
        step=1,
        disabled=not include_news
    )
    news_df = None
    stock_code = st.session_state.get("last_stock_code", "")
    if include_news:
        col_n1, col_n2 = st.columns([1, 3])
        with col_n1:
            fetch_news = st.button("æ‹‰å–æ–°é—»")
        with col_n2:
            st.caption("æç¤ºï¼šä»…ä¾›åˆ†æå‚è€ƒï¼Œæ–°é—»è¦†ç›–å¯èƒ½ä¸å®Œæ•´ã€‚")

        if fetch_news:
            with st.spinner("æ­£åœ¨æ‹‰å–ç›¸å…³æ–°é—»..."):
                news_df = _load_stock_news(stock_code, limit=news_limit)
                st.session_state.ai_news_df = news_df
                st.session_state.ai_news_stock = stock_code
                st.session_state.ai_news_limit = news_limit

        if (
            st.session_state.ai_news_df is not None
            and st.session_state.ai_news_stock == stock_code
            and st.session_state.ai_news_limit == news_limit
        ):
            news_df = st.session_state.ai_news_df

        if news_df is not None:
            if news_df.empty:
                st.info("æš‚æœªè·å–åˆ°ç›¸å…³æ–°é—»ã€‚")
            else:
                for _, row in news_df.head(5).iterrows():
                    title = row.get("æ–°é—»æ ‡é¢˜", "")
                    time = row.get("å‘å¸ƒæ—¶é—´", "")
                    st.markdown(f"- [{time}] {title}")

    if user_question:
        news_keywords = ["æ–°é—»", "å…¬å‘Š", "æ¶ˆæ¯", "æ”¿ç­–", "äº‹ä»¶", "æŠ¥é“"]
        if any(k in user_question for k in news_keywords) and not include_news:
            st.warning("æ£€æµ‹åˆ°æ–°é—»ç±»é—®é¢˜ï¼Œå»ºè®®å‹¾é€‰â€œåŒ…å«æœ€æ–°ç›¸å…³æ–°é—»â€å¹¶æ‹‰å–æ–°é—»ã€‚")

    with st.expander("ğŸ“Œ è¾“å…¥ç»™æ¨¡å‹çš„æ•°æ®é¢„è§ˆ", expanded=False):
        context = _build_context(news_df=news_df, include_news=include_news)
        st.json(_json_safe(context))

    col_g1, col_g2 = st.columns([1, 3])
    with col_g1:
        generate_btn = st.button("ç”Ÿæˆè§£è¯»", type="primary")
    with col_g2:
        st.caption("æç¤ºï¼šç”Ÿæˆä¼šè°ƒç”¨å¤–éƒ¨APIï¼Œé€Ÿåº¦å–å†³äºç½‘ç»œã€‚")

    if generate_btn:
        if include_news and news_df is None:
            with st.spinner("æ­£åœ¨æ‹‰å–ç›¸å…³æ–°é—»..."):
                news_df = _load_stock_news(stock_code, limit=news_limit)
                st.session_state.ai_news_df = news_df
                st.session_state.ai_news_stock = stock_code
                st.session_state.ai_news_limit = news_limit

        context = _build_context(news_df=news_df, include_news=include_news)
        stock_info = context.get("stock", {})
        session_id = f"{datetime.now().strftime('%Y%m%d-%H%M%S')}-{stock_info.get('code', '')}"
        system_prompt, user_prompt = _build_prompts(
            context=context,
            focus=focus,
            style=style,
            avoid_advice=avoid_advice,
            only_data=only_data,
            highlight_numbers=highlight_numbers,
            add_watchlist=add_watchlist,
            user_question=user_question
        )
        params_summary = _summarize_settings(
            focus=focus,
            style=style,
            avoid_advice=avoid_advice,
            only_data=only_data,
            highlight_numbers=highlight_numbers,
            add_watchlist=add_watchlist,
            user_question=user_question
        )
        with st.spinner("æ­£åœ¨ç”ŸæˆAIè§£è¯»..."):
            try:
                response = call_deepseek(
                    api_key=api_key,
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    temperature=temperature
                )
                entry = {
                    "ts": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "focus": focus,
                    "style": style,
                    "constraints": params_summary,
                    "user_question": user_question,
                    "temperature": temperature,
                    "response": response,
                    "system_prompt": system_prompt,
                    "context": _json_safe(context),
                    "stock_code": stock_info.get("code", ""),
                    "stock_name": stock_info.get("name", ""),
                    "requested_date": stock_info.get("requested_date"),
                    "actual_date": stock_info.get("actual_date"),
                    "session_id": session_id,
                    "followups": []
                }
                st.session_state.ai_history.append(entry)
                st.session_state.ai_last = entry
            except Exception as exc:
                st.error(f"è¯·æ±‚å¤±è´¥: {exc}")
                return

    if st.session_state.ai_last:
        st.markdown("### âœ… æœ€æ–°è§£è¯»")
        stock_label = f"{st.session_state.ai_last.get('stock_code', '')} {st.session_state.ai_last.get('stock_name', '')}".strip()
        date_label = st.session_state.ai_last.get("actual_date") or st.session_state.ai_last.get("requested_date") or "æœªçŸ¥æ—¥æœŸ"
        st.caption(
            f"{st.session_state.ai_last['ts']} | "
            f"{st.session_state.ai_last['focus']} | "
            f"{st.session_state.ai_last['style']} | "
            f"temp {st.session_state.ai_last['temperature']:.1f} | "
            f"æ ‡çš„: {stock_label or 'æœªçŸ¥'} | "
            f"æ—¥æœŸ: {date_label} | "
            f"ä¼šè¯: {st.session_state.ai_last.get('session_id', '--')}"
        )
        st.write(st.session_state.ai_last["response"])

        st.markdown("### ğŸ’¬ ç»§ç»­è¿½é—®")
        st.caption("è¿½é—®ä¼šåŸºäºâ€œæœ€æ–°è§£è¯»â€çš„åŒä¸€ä»½æ•°æ®å¿«ç…§ä¸æç¤ºè¯ç»§ç»­å›ç­”ã€‚")
        st.caption(f"å½“å‰ä¼šè¯: {st.session_state.ai_last.get('session_id', '--')}")
        followup = st.text_input("åŸºäºå½“å‰è§£è¯»ç»§ç»­æé—®", key="ai_followup")
        followup_btn = st.button("å‘é€è¿½é—®")
        if followup_btn:
            if not followup.strip():
                st.warning("è¯·è¾“å…¥è¿½é—®å†…å®¹ã€‚")
            else:
                with st.spinner("æ­£åœ¨è¿½é—®..."):
                    try:
                        follow_prompt = _build_followup_prompt(
                            context=st.session_state.ai_last["context"],
                            focus=st.session_state.ai_last["focus"],
                            constraints=st.session_state.ai_last["constraints"],
                            previous_answer=st.session_state.ai_last["response"],
                            followup=followup
                        )
                        follow_response = call_deepseek(
                            api_key=api_key,
                            system_prompt=st.session_state.ai_last["system_prompt"],
                            user_prompt=follow_prompt,
                            temperature=st.session_state.ai_last.get("temperature", 0.2)
                        )
                        st.session_state.ai_last["followups"].append(
                            {"q": followup, "a": follow_response}
                        )
                    except Exception as exc:
                        st.error(f"è¿½é—®å¤±è´¥: {exc}")
                        return

        if st.session_state.ai_last["followups"]:
            st.markdown("#### ğŸ§µ è¿½é—®è®°å½•")
            for item in st.session_state.ai_last["followups"][-5:]:
                st.markdown(f"**Q**: {item['q']}")
                st.markdown(f"**A**: {item['a']}")

    if st.session_state.ai_history:
        with st.expander("ğŸ—‚ï¸ å†å²è§£è¯»", expanded=False):
            for item in reversed(st.session_state.ai_history[-5:]):
                stock_label = f"{item.get('stock_code', '')} {item.get('stock_name', '')}".strip()
                date_label = item.get("actual_date") or item.get("requested_date") or "æœªçŸ¥æ—¥æœŸ"
                st.markdown(
                    f"**{item['ts']} | {item['focus']} | {item['style']} | "
                    f"{stock_label or 'æœªçŸ¥æ ‡çš„'} | {date_label} | ä¼šè¯ {item.get('session_id', '--')}**"
                )
                if item.get("user_question"):
                    st.caption(f"è¡¥å……é—®é¢˜: {item['user_question']}")
                st.write(item["response"])


def _build_context(
    news_df: Optional[pd.DataFrame] = None,
    include_news: bool = False
) -> Dict:
    df = st.session_state.df
    analysis = st.session_state.all_analysis
    quality = st.session_state.quality_report
    stock_code = st.session_state.get("last_stock_code", "")

    stock_provider = get_stock_provider()
    stock_name = stock_code
    try:
        res = stock_provider.search(stock_code, limit=1)
        if not res.empty:
            stock_name = res.iloc[0]["åç§°"]
    except Exception:
        pass

    actual_date = df.attrs.get("actual_date")
    requested_date = df.attrs.get("requested_date")

    flows = analysis.get("flows", {})
    timeseries = analysis.get("timeseries", {})
    indicators = analysis.get("indicators", {})
    anomalies = analysis.get("anomalies", {})
    large_orders = anomalies.get("large_orders", [])
    large_orders_sorted = sorted(large_orders, key=lambda x: x.get("amount", 0), reverse=True)
    top_orders = [
        {
            "time": str(o.get("time", "")),
            "amount": float(o.get("amount", 0)),
            "price": float(o.get("price", 0)),
            "type": o.get("type", "æœªçŸ¥"),
            "ratio": float(o.get("ratio", 0)),
        }
        for o in large_orders_sorted[:3]
    ]

    context = {
        "stock": {
            "code": stock_code,
            "name": stock_name,
            "requested_date": requested_date,
            "actual_date": actual_date,
            "data_quality_score": quality.get("quality_score", 0),
        },
        "price": {
            "open": timeseries.get("open_price"),
            "close": timeseries.get("close_price"),
            "high": timeseries.get("high_price"),
            "low": timeseries.get("low_price"),
            "change": timeseries.get("price_change"),
            "change_pct": timeseries.get("price_change_pct"),
            "amplitude": timeseries.get("amplitude"),
        },
        "liquidity": {
            "turnover_total": timeseries.get("turnover_total"),
            "volume_total": timeseries.get("volume_total"),
            "avg_price": timeseries.get("avg_price"),
        },
        "flow": {
            "large_net": flows.get("large_order_net_inflow"),
            "retail_net": flows.get("retail_net_inflow"),
            "large_ratio": flows.get("large_order_ratio"),
            "large_buy": flows.get("large_buy_amount"),
            "large_sell": flows.get("large_sell_amount"),
            "quality": flows.get("flow_quality", {}),
        },
        "indicators": {
            "vwap": indicators.get("vwap"),
            "price_vs_vwap": indicators.get("price_vs_vwap"),
            "ma5": indicators.get("ma5"),
            "ma10": indicators.get("ma10"),
            "is_above_vwap": indicators.get("is_above_vwap"),
            "is_above_ma5": indicators.get("is_above_ma5"),
            "is_above_ma10": indicators.get("is_above_ma10"),
        },
        "anomalies": {
            "large_order_count": anomalies.get("summary", {}).get("large_order_count", 0),
            "price_spike_count": anomalies.get("summary", {}).get("price_spike_count", 0),
            "volume_surge_count": anomalies.get("summary", {}).get("volume_surge_count", 0),
            "top_large_orders": top_orders,
        },
    }
    if include_news:
        context["news"] = _build_news_payload(news_df, stock_name)
    return context


def _build_prompts(
    context: Dict,
    focus: str,
    style: str,
    avoid_advice: bool,
    only_data: bool,
    highlight_numbers: bool,
    add_watchlist: bool,
    user_question: str
) -> Tuple[str, str]:
    constraints = []
    if avoid_advice:
        constraints.append("ä¸è¦ç»™å‡ºæ˜ç¡®ä¹°å–æŒ‡ä»¤æˆ–æ”¶ç›Šæ‰¿è¯º")
    if only_data:
        constraints.append("ä»…åŸºäºæä¾›çš„æ•°æ®è¿›è¡Œåˆ¤æ–­ï¼Œä¸è¦ç¼–é€ ")
    if highlight_numbers:
        constraints.append("å¿…é¡»å¼•ç”¨å…³é”®æ•°å€¼ä½œä¸ºä¾æ®")
    if add_watchlist:
        constraints.append("ç»™å‡ºå¯è§‚å¯Ÿçš„è§¦å‘æ¡ä»¶æˆ–å…³é”®å˜é‡")
    news_payload = context.get("news")
    if news_payload is not None:
        if news_payload.get("has_news"):
            constraints.append("å¦‚æœ‰æ–°é—»æ¡ç›®ï¼Œä»…åŸºäºæ–°é—»å†…å®¹æ¨æ–­æ½œåœ¨å½±å“ï¼Œé¿å…å¤¸å¤§")
        else:
            constraints.append("è‹¥æœªæä¾›æ–°é—»æ•°æ®éœ€æ˜ç¡®è¯´æ˜æ— æ³•åˆ¤æ–­æ–°é—»å½±å“")

    style_map = {
        "ç®€æ´": "4-6æ¡è¦ç‚¹ï¼Œå¥å­çŸ­",
        "ä¸“ä¸š": "åˆ†å°æ ‡é¢˜+è¦ç‚¹",
        "äº¤æ˜“å‘˜é£æ ¼": "å¼ºè°ƒç›˜ä¸­èŠ‚å¥ã€èµ„é‡‘æ–¹å‘ï¼Œè¯­æ°”ç´§å‡‘"
    }

    focus_map = {
        "èµ„é‡‘æµå‘è§£è¯»": [
            "ä¸»åŠ›/æ•£æˆ·å‡€æµå…¥æ–¹å‘ä¸å¼ºåº¦",
            "å¤§å•å æ¯”ä¸ä¸»åŠ›ä¹°å–é¢å·®å¼‚",
            "ç´¯è®¡å‡€æµå…¥æ˜¯å¦æŒç»­"
        ],
        "ç›˜ä¸­è¶‹åŠ¿ä¸èŠ‚å¥": [
            "å¼€æ”¶/é«˜ä½ä½ç½®ä¸æŒ¯å¹…",
            "VWAP/å‡çº¿åç¦»ä¸ç›˜ä¸­èŠ‚å¥",
            "ä¸Šæ¶¨åˆ†é’Ÿå æ¯”"
        ],
        "é£é™©ä¸å¼‚åŠ¨": [
            "ä»·æ ¼è·³è·ƒæ¬¡æ•°ä¸æ–¹å‘",
            "æˆäº¤é‡å¼‚å¸¸æ”¾å¤§",
            "å¤§å•å¼‚å¸¸é›†ä¸­æ—¶æ®µ"
        ],
        "ä¸»åŠ›è¡Œä¸ºå¤ç›˜": [
            "ä¸»åŠ›å‡€æµå…¥ä¸ä»·æ ¼èµ°åŠ¿æ˜¯å¦ä¸€è‡´",
            "ä¸»åŠ›ä¹°å–é¢å·®å¼‚",
            "ä¸»åŠ›å æ¯”ä¸å…³é”®æ—¶æ®µ"
        ],
    }

    system_prompt = (
        "ä½ æ˜¯ä¸“æ³¨äºAè‚¡æ—¥å†…èµ„é‡‘æµå‘åˆ†æçš„åŠ©æ‰‹ï¼Œåªèƒ½å›´ç»•äº¤æ˜“ä¸é‡‘èè¯é¢˜å›ç­”ã€‚"
        "å›å¤å¿…é¡»ç»“æ„åŒ–ï¼Œè¯­è¨€ç®€æ´ï¼Œé¿å…å‘æ•£ã€‚"
    )

    user_prompt = {
        "åˆ†æç›®æ ‡": focus,
        "è¾“å‡ºé£æ ¼": style_map.get(style, style),
        "çº¦æŸ": constraints,
        "é‡ç‚¹å…³æ³¨": focus_map.get(focus, []),
        "è¡¥å……é—®é¢˜": user_question or "æ— ",
        "æ•°æ®å¿«ç…§": _json_safe(context),
        "è¾“å‡ºæ ¼å¼": [
            "æ¦‚è§ˆ(1-2å¥)",
            "å…³é”®ä¾æ®(åˆ—å‡ºå…³é”®æ•°å€¼)",
            "é£é™©/ä¸ç¡®å®šæ€§",
            "è§‚å¯Ÿæ¸…å•(è§¦å‘æ¡ä»¶)"
        ],
    }

    return system_prompt, json.dumps(user_prompt, ensure_ascii=False, indent=2)


def _summarize_settings(
    focus: str,
    style: str,
    avoid_advice: bool,
    only_data: bool,
    highlight_numbers: bool,
    add_watchlist: bool,
    user_question: str
) -> List[str]:
    tags = [focus, style]
    if avoid_advice:
        tags.append("æ— ä¹°å–æŒ‡ä»¤")
    if only_data:
        tags.append("ä»…åŸºäºæ•°æ®")
    if highlight_numbers:
        tags.append("å¼ºè°ƒæ•°å€¼")
    if add_watchlist:
        tags.append("ç»™è§‚å¯Ÿæ¸…å•")
    if user_question:
        tags.append("å«è¡¥å……é—®é¢˜")
    return tags


def _build_followup_prompt(
    context: Dict,
    focus: str,
    constraints: List[str],
    previous_answer: str,
    followup: str
) -> str:
    focus_map = {
        "èµ„é‡‘æµå‘è§£è¯»": ["ä¸»åŠ›/æ•£æˆ·å‡€æµå…¥", "ç´¯è®¡å‡€æµå…¥", "å¤§å•å æ¯”"],
        "ç›˜ä¸­è¶‹åŠ¿ä¸èŠ‚å¥": ["ç›˜ä¸­èŠ‚å¥", "VWAP/å‡çº¿åç¦»", "æŒ¯å¹…"],
        "é£é™©ä¸å¼‚åŠ¨": ["ä»·æ ¼è·³è·ƒ", "æˆäº¤é‡æ¿€å¢", "å¤§å•å¼‚å¸¸"],
        "ä¸»åŠ›è¡Œä¸ºå¤ç›˜": ["ä¸»åŠ›å‡€æµå…¥ä¸ä»·æ ¼ä¸€è‡´æ€§", "ä¸»åŠ›ä¹°å–é¢å·®å¼‚"],
    }
    payload = {
        "ä»»åŠ¡": "åŸºäºå·²æœ‰è§£è¯»ç»§ç»­å›ç­”è¿½é—®ï¼Œä¿æŒé‡‘èäº¤æ˜“è¯­å¢ƒ",
        "åˆ†æç›®æ ‡": focus,
        "çº¦æŸ": constraints,
        "é‡ç‚¹å…³æ³¨": focus_map.get(focus, []),
        "å·²æœ‰è§£è¯»": previous_answer,
        "è¿½é—®": followup,
        "æ•°æ®å¿«ç…§": context,
        "è¾“å‡ºè¦æ±‚": [
            "ç›´æ¥å›ç­”é—®é¢˜",
            "å¼•ç”¨å…³é”®æ•°æ®",
            "ä¸æ‰©å±•åˆ°æ— å…³è¯é¢˜"
        ],
    }
    return json.dumps(payload, ensure_ascii=False, indent=2)


def _json_safe(value):
    if isinstance(value, dict):
        return {str(k): _json_safe(v) for k, v in value.items()}
    if isinstance(value, list):
        return [_json_safe(v) for v in value]
    if isinstance(value, (datetime, pd.Timestamp)):
        return value.isoformat()
    if isinstance(value, (str, int, float, bool)) or value is None:
        return value
    if hasattr(value, "item"):
        try:
            return value.item()
        except Exception:
            pass
    try:
        if pd.isna(value):
            return None
    except Exception:
        pass
    return str(value)
