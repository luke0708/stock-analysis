"""
æœªæ¥åŠŸèƒ½å ä½é¡µ (Mockups)
å±•ç¤ºå³å°†æ¨å‡ºçš„åŠŸèƒ½é¢„è§ˆå›¾ï¼Œè®©ç”¨æˆ·æ›´æœ‰å®æ„Ÿ
"""
import streamlit as st
import pandas as pd
import numpy as np
import json
from datetime import datetime, date, time, timedelta
from typing import Dict, List, Tuple, Optional

from stock_analysis.data.stock_list import get_stock_provider
from stock_analysis.data.providers.akshare_provider import AkShareProvider
from stock_analysis.analysis.ai_client import get_deepseek_key, call_deepseek
from stock_analysis.data.news_provider import StockNewsProvider


@st.cache_data(ttl=300)
def _load_stock_news(stock_code: str, limit: int) -> pd.DataFrame:
    return StockNewsProvider.get_stock_news(stock_code, limit=limit)


@st.cache_data(ttl=600)
def _load_daily_history(stock_code: str, end_date: date, window: int) -> pd.DataFrame:
    if not stock_code:
        return pd.DataFrame()
    provider = AkShareProvider()
    start_date = end_date - timedelta(days=window * 3)
    return provider.get_history_data(stock_code, start_date=start_date, end_date=end_date)


def _build_news_payload(news_df: Optional[pd.DataFrame], stock_name: str) -> Dict:
    if news_df is None or news_df.empty:
        return {
            "has_news": False,
            "source": "AkShare",
            "stock_name": stock_name,
            "items": [],
        }

    items = []
    for _, row in news_df.head(6).iterrows():
        items.append({
            "time": row.get("å‘å¸ƒæ—¶é—´", ""),
            "title": row.get("æ–°é—»æ ‡é¢˜", ""),
            "summary": row.get("æ–°é—»å†…å®¹", ""),
        })

    return {
        "has_news": True,
        "source": "AkShare",
        "stock_name": stock_name,
        "items": items,
        "latest_time": items[0]["time"] if items else None,
    }


def _drop_partial_daily(daily_df: pd.DataFrame, target_date: date) -> pd.DataFrame:
    if daily_df is None or daily_df.empty:
        return daily_df
    df = daily_df.copy()
    date_col = None
    for col in ["æ—¥æœŸ", "date", "æ—¶é—´", "trade_date"]:
        if col in df.columns:
            date_col = col
            break
    if not date_col:
        return daily_df
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    df = df.dropna(subset=[date_col])
    return df[df[date_col].dt.date != target_date]


def _parse_date_value(date_str: Optional[str]) -> Optional[date]:
    if not date_str:
        return None
    for fmt in ("%Y%m%d", "%Y-%m-%d", "%Y/%m/%d"):
        try:
            return datetime.strptime(date_str, fmt).date()
        except ValueError:
            continue
    try:
        parsed = pd.to_datetime(date_str, errors="coerce")
        if pd.isna(parsed):
            return None
        return parsed.date()
    except Exception:
        return None


def _safe_number(value) -> Optional[float]:
    if value is None:
        return None
    try:
        if isinstance(value, float) and np.isnan(value):
            return None
    except Exception:
        pass
    try:
        return float(value)
    except Exception:
        return None


def _build_daily_series(daily_df: pd.DataFrame, limit: int) -> List[Dict]:
    if daily_df is None or daily_df.empty:
        return []

    df = daily_df.copy()
    date_col = None
    for col in ["æ—¥æœŸ", "date", "æ—¶é—´", "trade_date"]:
        if col in df.columns:
            date_col = col
            break
    if not date_col:
        return []

    col_map = {
        "open": "å¼€ç›˜",
        "high": "æœ€é«˜",
        "low": "æœ€ä½",
        "close": "æ”¶ç›˜",
        "volume": "æˆäº¤é‡",
        "amount": "æˆäº¤é¢",
    }
    df = df.rename(columns=col_map)
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    df = df.dropna(subset=[date_col]).sort_values(date_col)

    for col in ["æ”¶ç›˜", "æœ€é«˜", "æœ€ä½", "æˆäº¤é‡", "æˆäº¤é¢"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    if "æ”¶ç›˜" not in df.columns:
        return []

    df["return_pct"] = df["æ”¶ç›˜"].pct_change() * 100
    for ma in [5, 10, 20]:
        df[f"ma{ma}"] = df["æ”¶ç›˜"].rolling(ma).mean()

    df_tail = df.tail(limit)
    series = []
    for _, row in df_tail.iterrows():
        date_val = row.get(date_col)
        date_str = date_val.strftime("%Y-%m-%d") if pd.notna(date_val) else ""
        series.append(
            {
                "date": date_str,
                "high": _safe_number(row.get("æœ€é«˜")),
                "low": _safe_number(row.get("æœ€ä½")),
                "close": _safe_number(row.get("æ”¶ç›˜")),
                "return_pct": _safe_number(row.get("return_pct")),
                "volume": _safe_number(row.get("æˆäº¤é‡")),
                "ma5": _safe_number(row.get("ma5")),
                "ma10": _safe_number(row.get("ma10")),
                "ma20": _safe_number(row.get("ma20")),
            }
        )
    return series


def _build_daily_trend(daily_df: pd.DataFrame, limit: int) -> Dict:
    if daily_df is None or daily_df.empty:
        return {}

    df = daily_df.copy()
    date_col = None
    for col in ["æ—¥æœŸ", "date", "æ—¶é—´", "trade_date"]:
        if col in df.columns:
            date_col = col
            break
    if not date_col:
        return {}

    col_map = {
        "open": "å¼€ç›˜",
        "high": "æœ€é«˜",
        "low": "æœ€ä½",
        "close": "æ”¶ç›˜",
        "volume": "æˆäº¤é‡",
        "amount": "æˆäº¤é¢",
    }
    df = df.rename(columns=col_map)
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    df = df.dropna(subset=[date_col]).sort_values(date_col)

    if "æ”¶ç›˜" not in df.columns:
        return {}

    df["æ”¶ç›˜"] = pd.to_numeric(df["æ”¶ç›˜"], errors="coerce")
    if "æœ€é«˜" in df.columns:
        df["æœ€é«˜"] = pd.to_numeric(df["æœ€é«˜"], errors="coerce")
    if "æœ€ä½" in df.columns:
        df["æœ€ä½"] = pd.to_numeric(df["æœ€ä½"], errors="coerce")
    if "æˆäº¤é‡" in df.columns:
        df["æˆäº¤é‡"] = pd.to_numeric(df["æˆäº¤é‡"], errors="coerce")
    if "æˆäº¤é¢" in df.columns:
        df["æˆäº¤é¢"] = pd.to_numeric(df["æˆäº¤é¢"], errors="coerce")

    df_tail = df.tail(limit).copy()
    available_days = len(df_tail)
    if available_days < 2:
        return {}

    df_tail["return_pct"] = df_tail["æ”¶ç›˜"].pct_change() * 100
    for ma in [5, 10, 20]:
        df_tail[f"ma{ma}"] = df_tail["æ”¶ç›˜"].rolling(ma).mean()

    close_first = df_tail["æ”¶ç›˜"].iloc[0]
    close_last = df_tail["æ”¶ç›˜"].iloc[-1]
    return_pct = (
        (close_last / close_first - 1) * 100
        if pd.notna(close_first) and close_first != 0
        else None
    )

    daily_volatility = df_tail["return_pct"].std()

    ma5_last = df_tail["ma5"].iloc[-1]
    ma10_last = df_tail["ma10"].iloc[-1]
    ma20_last = df_tail["ma20"].iloc[-1]

    if pd.notna(ma5_last) and pd.notna(ma10_last) and pd.notna(ma20_last):
        if ma5_last > ma10_last > ma20_last:
            ma_alignment = "bullish"
        elif ma5_last < ma10_last < ma20_last:
            ma_alignment = "bearish"
        else:
            ma_alignment = "mixed"
    else:
        ma_alignment = "unknown"

    close_vs_ma20_pct = (
        (close_last - ma20_last) / ma20_last * 100
        if pd.notna(ma20_last) and ma20_last != 0
        else None
    )

    ma20_series = df_tail["ma20"].dropna()
    if len(ma20_series) >= 2 and ma20_series.iloc[0] != 0:
        ma20_slope_pct = (ma20_series.iloc[-1] / ma20_series.iloc[0] - 1) * 100
    else:
        ma20_slope_pct = None

    rolling_max = df_tail["æ”¶ç›˜"].cummax()
    drawdown = (df_tail["æ”¶ç›˜"] - rolling_max) / rolling_max.replace(0, np.nan)
    max_drawdown = drawdown.min() * 100 if not drawdown.empty else None

    volume_change_pct = None
    if "æˆäº¤é‡" in df_tail.columns and df_tail["æˆäº¤é‡"].notna().sum() >= 6:
        recent = df_tail["æˆäº¤é‡"].tail(5).mean()
        prev = df_tail["æˆäº¤é‡"].iloc[-10:-5].mean() if available_days >= 10 else df_tail["æˆäº¤é‡"].head(5).mean()
        if pd.notna(prev) and prev != 0:
            volume_change_pct = (recent / prev - 1) * 100

    range_high = None
    range_low = None
    if "æœ€é«˜" in df_tail.columns:
        range_high = df_tail["æœ€é«˜"].max()
    if "æœ€ä½" in df_tail.columns:
        range_low = df_tail["æœ€ä½"].min()
    range_mid = None
    range_width_pct = None
    if (
        range_high is not None
        and range_low is not None
        and pd.notna(range_high)
        and pd.notna(range_low)
    ):
        range_mid = (range_high + range_low) / 2
        if pd.notna(close_last) and close_last != 0:
            range_width_pct = (range_high - range_low) / close_last * 100

    atr_14 = None
    if {"æœ€é«˜", "æœ€ä½", "æ”¶ç›˜"}.issubset(df_tail.columns):
        prev_close = df_tail["æ”¶ç›˜"].shift(1)
        tr = pd.concat(
            [
                (df_tail["æœ€é«˜"] - df_tail["æœ€ä½"]).abs(),
                (df_tail["æœ€é«˜"] - prev_close).abs(),
                (df_tail["æœ€ä½"] - prev_close).abs(),
            ],
            axis=1,
        ).max(axis=1)
        atr_14 = tr.tail(14).mean()

    trend_label = "range"
    if return_pct is not None:
        if return_pct > 5 and ma_alignment == "bullish":
            trend_label = "up"
        elif return_pct < -5 and ma_alignment == "bearish":
            trend_label = "down"

    strength = "weak"
    if return_pct is not None and abs(return_pct) >= 8:
        strength = "strong"
    elif return_pct is not None and abs(return_pct) >= 3:
        strength = "medium"

    return {
        "window_days": available_days,
        "return_pct": _safe_number(return_pct),
        "volatility_pct": _safe_number(daily_volatility),
        "max_drawdown_pct": _safe_number(max_drawdown),
        "ma5": _safe_number(ma5_last),
        "ma10": _safe_number(ma10_last),
        "ma20": _safe_number(ma20_last),
        "ma_alignment": ma_alignment,
        "ma20_slope_pct": _safe_number(ma20_slope_pct),
        "close_vs_ma20_pct": _safe_number(close_vs_ma20_pct),
        "volume_change_pct": _safe_number(volume_change_pct),
        "range_support": _safe_number(range_low),
        "range_mid": _safe_number(range_mid),
        "range_resistance": _safe_number(range_high),
        "range_width_pct": _safe_number(range_width_pct),
        "atr_14": _safe_number(atr_14),
        "trend_label": trend_label,
        "trend_strength": strength,
    }


def _build_tick_window_series(tick_context: Dict, limit: int) -> Tuple[List[Dict], Optional[int]]:
    if not tick_context:
        return [], None

    window_df = None
    window_minutes = None
    for minutes, key in [(5, "window_5m"), (1, "window_1m"), (10, "window_10m")]:
        candidate = tick_context.get(key)
        if candidate is not None and not candidate.empty:
            window_df = candidate
            window_minutes = minutes
            break

    if window_df is None or window_df.empty:
        return [], window_minutes

    df_tail = window_df.tail(limit)
    series = []
    for _, row in df_tail.iterrows():
        time_window = row.get("time_window")
        if not time_window and "æ—¶é—´" in df_tail.columns:
            time_window = row.get("æ—¶é—´")
        series.append(
            {
                "time_window": str(time_window) if time_window is not None else "",
                "buy_amount": _safe_number(row.get("buy_amount")),
                "sell_amount": _safe_number(row.get("sell_amount")),
                "net_inflow": _safe_number(row.get("net_inflow")),
                "turnover": _safe_number(row.get("turnover")),
                "ofi": _safe_number(row.get("ofi")),
                "trade_count": _safe_number(row.get("trade_count")),
                "range_pct": _safe_number(row.get("range_pct")),
            }
        )
    return series, window_minutes


def _extract_latest_time(df: pd.DataFrame) -> Optional[datetime]:
    if df is None or df.empty:
        return None
    for col in ["æ—¶é—´", "datetime", "time", "æˆäº¤æ—¶é—´"]:
        if col in df.columns:
            value = pd.to_datetime(df[col], errors="coerce")
            value = value.dropna()
            if not value.empty:
                return value.iloc[-1].to_pydatetime()
    return None


def _calc_trading_progress(as_of: datetime) -> Dict:
    sessions = [(time(9, 30), time(11, 30)), (time(13, 0), time(15, 0))]
    total_minutes = 240
    elapsed = 0
    for start, end in sessions:
        if as_of.time() <= start:
            continue
        session_end = end if as_of.time() >= end else as_of.time()
        if session_end <= start:
            continue
        elapsed += int(
            (datetime.combine(as_of.date(), session_end) - datetime.combine(as_of.date(), start)).total_seconds() / 60
        )
    progress = min(max(elapsed / total_minutes, 0.0), 1.0)
    return {
        "elapsed_minutes": elapsed,
        "total_minutes": total_minutes,
        "progress": round(progress, 4),
    }


def _build_today_partial(
    df: pd.DataFrame,
    timeseries: Dict,
    indicators: Dict,
    tick_context: Optional[Dict],
    analysis_day: date,
) -> Dict:
    latest_dt = _extract_latest_time(df)
    if latest_dt is None:
        latest_dt = datetime.combine(analysis_day, datetime.now().time())

    is_today = analysis_day == datetime.now().date()
    is_partial = is_today and latest_dt.time() < time(15, 0)

    data_scope = {
        "as_of": latest_dt.strftime("%Y-%m-%d %H:%M:%S"),
        "is_partial": is_partial,
        "source_granularity": tick_context.get("flow_summary", {}).get("flow_quality", {}).get("data_granularity")
        if tick_context
        else "minute",
    }
    if is_today:
        data_scope.update(_calc_trading_progress(latest_dt))

    return {
        "scope": data_scope,
        "price": {
            "open": timeseries.get("open_price"),
            "close": timeseries.get("close_price"),
            "high": timeseries.get("high_price"),
            "low": timeseries.get("low_price"),
            "change_pct": timeseries.get("price_change_pct"),
        },
        "liquidity": {
            "turnover_total": timeseries.get("turnover_total"),
            "volume_total": timeseries.get("volume_total"),
            "vwap": indicators.get("vwap"),
        },
    }

def show_multi_stock_compare():
    st.header("âš–ï¸ å¤šè‚¡ç¥¨å¯¹æ¯”åˆ†æ (Coming Soon)")
    st.info("ğŸš§ æ­¤åŠŸèƒ½å°†åœ¨ v1.2 ç‰ˆæœ¬ä¸Šçº¿")
    
    col1, col2 = st.columns([1, 1])
    with col1:
        st.subheader("åŠŸèƒ½é¢„è§ˆ")
        st.markdown("""
        - **å¤šç»´å åŠ **: åŒæ—¶æŸ¥çœ‹æœ€å¤š 5 åªè‚¡ç¥¨çš„èµ°åŠ¿
        - **ç›¸å¯¹æ”¶ç›Š**: ä»¥æŸæ—¥ä¸ºåŸºå‡†æŸ¥çœ‹ç›¸å¯¹æ¶¨è·Œå¹…
        - **èµ„é‡‘æµå¯¹æ¯”**: æ¨ªå‘æ¯”è¾ƒè°çš„ä¸»åŠ›ä»‹å…¥æ›´æ·±
        """)
        
    with col2:
        # Mock chart
        dates = pd.date_range(end=pd.Timestamp.now(), periods=100)
        # Generate trend data (cumulative sum) + ensure no infinities
        np.random.seed(42)  # Fixed seed for stability
        data = pd.DataFrame(
            np.random.randn(100, 3).cumsum(0),
            index=dates,
            columns=['è´µå·èŒ…å° (Mock)', 'å®å¾·æ—¶ä»£ (Mock)', 'æ‹›å•†é“¶è¡Œ (Mock)']
        )
        # Add offset to avoid 0/negative if using log scale (though line_chart defaults to linear)
        data = data + 100 
        
        st.line_chart(data)

def show_backtesting():
    st.header("ğŸ§ª ç­–ç•¥å›æµ‹å®éªŒå®¤ (Coming Soon)")
    st.warning("ğŸš§ æ­¤åŠŸèƒ½å°†åœ¨ v1.3 ç‰ˆæœ¬ä¸Šçº¿")
    
    st.markdown("### é¢„è®¾ç­–ç•¥é…ç½®")
    c1, c2, c3 = st.columns(3)
    c1.selectbox("äº¤æ˜“ç­–ç•¥", ["åŒå‡çº¿äº¤å‰", "RSIè¶…ä¹°è¶…å–", "ç½‘æ ¼äº¤æ˜“"])
    c2.date_input("å›æµ‹å¼€å§‹", value=pd.to_datetime("2023-01-01"))
    c3.number_input("åˆå§‹èµ„é‡‘", value=100000)
    
    st.button("å¼€å§‹å›æµ‹ (æ¼”ç¤ºæŒ‰é’®)", disabled=True)
    
    st.markdown("### é¢„æœŸå›æµ‹æŠ¥å‘Š")
    st.write("ğŸ“ˆ å¹´åŒ–æ”¶ç›Šç‡: 15.2% | ğŸ“‰ æœ€å¤§å›æ’¤: -8.5% | ğŸ¯ èƒœç‡: 58%")
    
def show_global_markets():
    st.header("ğŸŒ å…¨çƒå¸‚åœºæ¦‚è§ˆ (Coming Soon)")
    st.success("ğŸš§ é•¿æœŸè§„åˆ’åŠŸèƒ½ (v2.2)")
    
    cols = st.columns(4)
    cols[0].metric("çº³æ–¯è¾¾å…‹", "14,890.30", "+1.2%")
    cols[1].metric("æ’ç”ŸæŒ‡æ•°", "16,500.00", "-0.5%")
    cols[2].metric("æ—¥ç»225", "35,000.00", "+0.8%")
    cols[3].metric("æ ‡æ™®500", "4,780.00", "+0.9%")
    
    st.caption("*ä»¥ä¸Šæ•°æ®ä»…ä¸ºé™æ€æ¼”ç¤º*")

def show_ai_analysis():
    st.header("ğŸ¤– AI æ™ºèƒ½æŠ•é¡¾")
    st.caption("ä¸“æ³¨äºAè‚¡èµ„é‡‘æµå‘ä¸æ—¥å†…äº¤æ˜“è§£è¯»ï¼Œè¾“å‡ºä¸ºç»“æ„åŒ–ç»“è®º")

    api_key, api_key_name = get_deepseek_key()
    if not api_key:
        st.warning("æœªæ£€æµ‹åˆ° DeepSeek API Keyï¼Œè¯·åœ¨ .env ä¸­é…ç½®åå†ä½¿ç”¨ã€‚")
        st.code("DEEPSEEK_API_KEY=ä½ çš„key", language="bash")
        return

    st.caption(f"å½“å‰ä½¿ç”¨ç¯å¢ƒå˜é‡: {api_key_name}")

    if "df" not in st.session_state or st.session_state.df is None:
        st.info("è¯·å…ˆåœ¨â€œä¸ªè‚¡èµ„é‡‘æµå‘â€é¡µé¢å®Œæˆä¸€æ¬¡åˆ†æï¼Œä»¥ä¾¿ç”Ÿæˆæ›´å‡†ç¡®çš„ AI è§£è¯»ã€‚")
        return

    if "ai_history" not in st.session_state:
        st.session_state.ai_history = []
    if "ai_last" not in st.session_state:
        st.session_state.ai_last = None
    if "ai_news_df" not in st.session_state:
        st.session_state.ai_news_df = None
    if "ai_news_stock" not in st.session_state:
        st.session_state.ai_news_stock = ""
    if "ai_news_limit" not in st.session_state:
        st.session_state.ai_news_limit = 0

    with st.expander("â„¹ï¸ è¾“å…¥æ•°æ®è¯´æ˜", expanded=False):
        st.write(
            "æ¨¡å‹è¾“å…¥æ¥è‡ªæœ€è¿‘ä¸€æ¬¡ä¸ªè‚¡åˆ†æç»“æœï¼ŒåŒ…å«ä»·æ ¼ã€èµ„é‡‘æµã€æŠ€æœ¯æŒ‡æ ‡ä¸å¼‚åŠ¨ç»Ÿè®¡ã€‚"
            "åŸå§‹æ•°æ®å·²ç»è¿‡ DataCleaner æ¸…æ´—ï¼ˆä¿®å¤ç¼ºå¤±å€¼/å¼‚å¸¸å€¼ã€æ ‡å‡†åŒ–ç±»å‹ï¼‰ã€‚"
            "æ¨¡å‹è¾“å‡ºé«˜åº¦ä¾èµ–è¿™äº›ç»“æ„åŒ–æ•°æ®ï¼Œå› æ­¤åˆ‡æ¢æ—¥æœŸæˆ–è‚¡ç¥¨ä¼šæ˜¾è‘—å½±å“è§£è¯»ã€‚"
        )

    st.markdown("### ğŸ›ï¸ é¢„è®¾æ¡£ä½")
    preset = st.radio(
        "é€‰æ‹©æ¡£ä½",
        ["è‡ªå®šä¹‰", "ä»·æ ¼åŒºé—´å»ºè®®(è¶‹åŠ¿)", "èµ„é‡‘æµé£é™©æç¤º(å½“æ—¥)"],
        horizontal=True,
        help="é¢„è®¾æ¡£ä½ä¼šé”å®šåˆ†æä¾§é‡ç‚¹ä¸è¾“å‡ºæ–¹å¼ï¼Œé¿å…é…ç½®å†²çªã€‚"
    )

    preset_config = {
        "ä»·æ ¼åŒºé—´å»ºè®®(è¶‹åŠ¿)": {
            "mode": "range",
            "focus": "ç›˜ä¸­è¶‹åŠ¿ä¸èŠ‚å¥",
            "style": "ä¸“ä¸š",
            "advice_mode": "è¡ŒåŠ¨æ¨¡å¼",
            "only_data": True,
            "highlight_numbers": True,
            "add_watchlist": True,
            "include_news": False,
        },
        "èµ„é‡‘æµé£é™©æç¤º(å½“æ—¥)": {
            "mode": "risk",
            "focus": "é£é™©ä¸å¼‚åŠ¨",
            "style": "äº¤æ˜“å‘˜é£æ ¼",
            "advice_mode": "ç»“è®ºæ¨¡å¼",
            "only_data": True,
            "highlight_numbers": True,
            "add_watchlist": True,
            "include_news": False,
        },
    }

    preset_mode = "custom"
    if preset != "è‡ªå®šä¹‰":
        config = preset_config[preset]
        preset_mode = config["mode"]
        focus = config["focus"]
        style = config["style"]
        advice_mode = config["advice_mode"]
        only_data = config["only_data"]
        highlight_numbers = config["highlight_numbers"]
        add_watchlist = config["add_watchlist"]
        include_news = config["include_news"]

        st.info(
            f"å·²é”å®šé…ç½®ï¼šä¾§é‡ç‚¹={focus}ï¼Œé£æ ¼={style}ï¼Œæ¨¡å¼={advice_mode}ï¼Œ"
            f"åªåŸºäºæ•°æ®={only_data}ï¼Œçªå‡ºæ•°å€¼={highlight_numbers}ï¼Œè§‚å¯Ÿæ¸…å•={add_watchlist}ï¼Œæ–°é—»={include_news}"
        )
    else:
        st.markdown("### ğŸ¯ åˆ†æç›®æ ‡")
        focus = st.radio(
            "è¯·é€‰æ‹©åˆ†æä¾§é‡ç‚¹",
            ["èµ„é‡‘æµå‘è§£è¯»", "ç›˜ä¸­è¶‹åŠ¿ä¸èŠ‚å¥", "é£é™©ä¸å¼‚åŠ¨", "ä¸»åŠ›è¡Œä¸ºå¤ç›˜"],
            horizontal=True,
            help="åˆ‡æ¢ä¾§é‡ç‚¹ä¼šæ”¹å˜æç¤ºè¯ï¼Œä½†éœ€è¦ç‚¹å‡»â€œç”Ÿæˆè§£è¯»â€æ‰ä¼šæ›´æ–°ç»“æœã€‚"
        )

        style = st.radio(
            "è¾“å‡ºé£æ ¼",
            ["ç®€æ´", "ä¸“ä¸š", "äº¤æ˜“å‘˜é£æ ¼"],
            horizontal=True,
            help="ç®€æ´=è¦ç‚¹çŸ­å¥ï¼›ä¸“ä¸š=åˆ†å°æ ‡é¢˜ï¼›äº¤æ˜“å‘˜=æ›´å¼ºè°ƒç›˜ä¸­èŠ‚å¥ã€‚"
        )

        advice_mode = st.radio(
            "è¾“å‡ºæ¨¡å¼",
            ["åˆ†ææ¨¡å¼", "ç»“è®ºæ¨¡å¼", "è¡ŒåŠ¨æ¨¡å¼"],
            horizontal=True,
            help="è¡ŒåŠ¨æ¨¡å¼ä¼šç»™å‡ºæ¡ä»¶è§¦å‘å»ºè®®ï¼Œä¸åšæ”¶ç›Šæ‰¿è¯ºã€‚"
        )

        col1, col2 = st.columns(2)
        with col1:
            only_data = st.checkbox(
                "åªåŸºäºç»™å®šæ•°æ®",
                value=True,
                help="åªä½¿ç”¨å½“å‰åˆ†æç»“æœï¼Œä¸å¼•å…¥å¤–éƒ¨ä¿¡æ¯ã€‚"
            )
        with col2:
            highlight_numbers = st.checkbox(
                "çªå‡ºå…³é”®æ•°å€¼",
                value=True,
                help="å¿…é¡»å¼•ç”¨å…³é”®æ•°æ®ä½œä¸ºè®ºæ®ã€‚"
            )
            add_watchlist = st.checkbox(
                "ç»™å‡ºè§‚å¯Ÿæ¸…å•",
                value=True,
                help="åˆ—å‡ºåç»­å…³æ³¨çš„è§¦å‘æ¡ä»¶æˆ–å…³é”®å˜é‡ã€‚"
            )

    temperature = st.slider(
        "è¾“å‡ºå¤šæ ·æ€§ (temperature)",
        min_value=0.0,
        max_value=0.8,
        value=0.2,
        step=0.1,
        help="æ•°å€¼è¶Šä½è¶Šç¨³å®šã€è¶Šæ¥è¿‘ç¡®å®šè¾“å‡ºï¼›æ•°å€¼è¶Šé«˜è¶Šå¤šæ ·åŒ–ã€‚"
    )

    user_question = st.text_area(
        "è¡¥å……é—®é¢˜ï¼ˆå¯é€‰ï¼‰",
        placeholder="ä¾‹å¦‚ï¼šä»Šå¤©ä¸»åŠ›å¸ç­¹æ˜¯å¦æ˜æ˜¾ï¼ŸçŸ­çº¿æœ‰å“ªäº›é£é™©ç‚¹ï¼Ÿ",
        help="è¡¥å……å…·ä½“é—®é¢˜ä¼šæ”¹å˜è§£è¯»é‡ç‚¹ï¼›è‹¥æ¶‰åŠæ–°é—»ï¼Œè¯·å¼€å¯ä¸‹æ–¹â€œåŒ…å«æœ€æ–°ç›¸å…³æ–°é—»â€ã€‚"
    )

    st.markdown("### ğŸ“° æ–°é—»è¡¥å……ï¼ˆå¯é€‰ï¼‰")
    if preset == "è‡ªå®šä¹‰":
        include_news = st.checkbox(
            "åŒ…å«æœ€æ–°ç›¸å…³æ–°é—»",
            value=False,
            help="ä» AkShare æ‹‰å–ç›¸å…³æ–°é—»ï¼Œå¯èƒ½æœ‰å»¶è¿Ÿæˆ–ç¼ºå¤±ã€‚"
        )
    else:
        st.caption("é¢„è®¾æ¡£ä½é»˜è®¤ä¸åŒ…å«æ–°é—»ï¼Œé¿å…å¹²æ‰°è¶‹åŠ¿/èµ„é‡‘æµåˆ¤æ–­ã€‚")
    news_limit = st.slider(
        "æ–°é—»æ¡æ•°",
        min_value=3,
        max_value=12,
        value=6,
        step=1,
        disabled=not include_news
    )
    news_df = None
    stock_code = st.session_state.get("last_stock_code", "")
    if include_news:
        col_n1, col_n2 = st.columns([1, 3])
        with col_n1:
            fetch_news = st.button("æ‹‰å–æ–°é—»")
        with col_n2:
            st.caption("æç¤ºï¼šä»…ä¾›åˆ†æå‚è€ƒï¼Œæ–°é—»è¦†ç›–å¯èƒ½ä¸å®Œæ•´ã€‚")

        if fetch_news:
            with st.spinner("æ­£åœ¨æ‹‰å–ç›¸å…³æ–°é—»..."):
                news_df = _load_stock_news(stock_code, limit=news_limit)
                st.session_state.ai_news_df = news_df
                st.session_state.ai_news_stock = stock_code
                st.session_state.ai_news_limit = news_limit

        if (
            st.session_state.ai_news_df is not None
            and st.session_state.ai_news_stock == stock_code
            and st.session_state.ai_news_limit == news_limit
        ):
            news_df = st.session_state.ai_news_df

        if news_df is not None:
            if news_df.empty:
                st.info("æš‚æœªè·å–åˆ°ç›¸å…³æ–°é—»ã€‚")
            else:
                for _, row in news_df.head(5).iterrows():
                    title = row.get("æ–°é—»æ ‡é¢˜", "")
                    time = row.get("å‘å¸ƒæ—¶é—´", "")
                    st.markdown(f"- [{time}] {title}")

    if user_question:
        news_keywords = ["æ–°é—»", "å…¬å‘Š", "æ¶ˆæ¯", "æ”¿ç­–", "äº‹ä»¶", "æŠ¥é“"]
        if any(k in user_question for k in news_keywords) and not include_news:
            st.warning("æ£€æµ‹åˆ°æ–°é—»ç±»é—®é¢˜ï¼Œå»ºè®®å‹¾é€‰â€œåŒ…å«æœ€æ–°ç›¸å…³æ–°é—»â€å¹¶æ‹‰å–æ–°é—»ã€‚")

    with st.expander("ğŸ“Œ è¾“å…¥ç»™æ¨¡å‹çš„æ•°æ®é¢„è§ˆ", expanded=False):
        context = _build_context(news_df=news_df, include_news=include_news)
        st.json(_json_safe(context))

    col_g1, col_g2 = st.columns([1, 3])
    with col_g1:
        generate_btn = st.button("ç”Ÿæˆè§£è¯»", type="primary")
    with col_g2:
        st.caption("æç¤ºï¼šç”Ÿæˆä¼šè°ƒç”¨å¤–éƒ¨APIï¼Œé€Ÿåº¦å–å†³äºç½‘ç»œã€‚")

    if generate_btn:
        if include_news and news_df is None:
            with st.spinner("æ­£åœ¨æ‹‰å–ç›¸å…³æ–°é—»..."):
                news_df = _load_stock_news(stock_code, limit=news_limit)
                st.session_state.ai_news_df = news_df
                st.session_state.ai_news_stock = stock_code
                st.session_state.ai_news_limit = news_limit

        context = _build_context(news_df=news_df, include_news=include_news)
        stock_info = context.get("stock", {})
        session_id = f"{datetime.now().strftime('%Y%m%d-%H%M%S')}-{stock_info.get('code', '')}"
        system_prompt, user_prompt = _build_prompts(
            context=context,
            focus=focus,
            style=style,
            advice_mode=advice_mode,
            preset_mode=preset_mode,
            only_data=only_data,
            highlight_numbers=highlight_numbers,
            add_watchlist=add_watchlist,
            user_question=user_question
        )
        params_summary = _summarize_settings(
            focus=focus,
            style=style,
            advice_mode=advice_mode,
            preset_mode=preset_mode,
            only_data=only_data,
            highlight_numbers=highlight_numbers,
            add_watchlist=add_watchlist,
            user_question=user_question
        )
        with st.spinner("æ­£åœ¨ç”ŸæˆAIè§£è¯»..."):
            try:
                response = call_deepseek(
                    api_key=api_key,
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    temperature=temperature
                )
                entry = {
                    "ts": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "focus": focus,
                    "style": style,
                    "advice_mode": advice_mode,
                    "preset_mode": preset_mode,
                    "constraints": params_summary,
                    "user_question": user_question,
                    "temperature": temperature,
                    "response": response,
                    "system_prompt": system_prompt,
                    "context": _json_safe(context),
                    "stock_code": stock_info.get("code", ""),
                    "stock_name": stock_info.get("name", ""),
                    "requested_date": stock_info.get("requested_date"),
                    "actual_date": stock_info.get("actual_date"),
                    "session_id": session_id,
                    "followups": []
                }
                st.session_state.ai_history.append(entry)
                st.session_state.ai_last = entry
            except Exception as exc:
                st.error(f"è¯·æ±‚å¤±è´¥: {exc}")
                return

    if st.session_state.ai_last:
        st.markdown("### âœ… æœ€æ–°è§£è¯»")
        stock_label = f"{st.session_state.ai_last.get('stock_code', '')} {st.session_state.ai_last.get('stock_name', '')}".strip()
        date_label = st.session_state.ai_last.get("actual_date") or st.session_state.ai_last.get("requested_date") or "æœªçŸ¥æ—¥æœŸ"
        st.caption(
            f"{st.session_state.ai_last['ts']} | "
            f"{st.session_state.ai_last['focus']} | "
            f"{st.session_state.ai_last['style']} | "
            f"{st.session_state.ai_last.get('advice_mode', '')} | "
            f"{st.session_state.ai_last.get('preset_mode', '')} | "
            f"temp {st.session_state.ai_last['temperature']:.1f} | "
            f"æ ‡çš„: {stock_label or 'æœªçŸ¥'} | "
            f"æ—¥æœŸ: {date_label} | "
            f"ä¼šè¯: {st.session_state.ai_last.get('session_id', '--')}"
        )
        st.write(st.session_state.ai_last["response"])

        st.markdown("### ğŸ’¬ ç»§ç»­è¿½é—®")
        st.caption("è¿½é—®ä¼šåŸºäºâ€œæœ€æ–°è§£è¯»â€çš„åŒä¸€ä»½æ•°æ®å¿«ç…§ä¸æç¤ºè¯ç»§ç»­å›ç­”ã€‚")
        st.caption(f"å½“å‰ä¼šè¯: {st.session_state.ai_last.get('session_id', '--')}")
        followup = st.text_input("åŸºäºå½“å‰è§£è¯»ç»§ç»­æé—®", key="ai_followup")
        followup_btn = st.button("å‘é€è¿½é—®")
        if followup_btn:
            if not followup.strip():
                st.warning("è¯·è¾“å…¥è¿½é—®å†…å®¹ã€‚")
            else:
                with st.spinner("æ­£åœ¨è¿½é—®..."):
                    try:
                        follow_prompt = _build_followup_prompt(
                            context=st.session_state.ai_last["context"],
                            focus=st.session_state.ai_last["focus"],
                            constraints=st.session_state.ai_last["constraints"],
                            previous_answer=st.session_state.ai_last["response"],
                            followup=followup
                        )
                        follow_response = call_deepseek(
                            api_key=api_key,
                            system_prompt=st.session_state.ai_last["system_prompt"],
                            user_prompt=follow_prompt,
                            temperature=st.session_state.ai_last.get("temperature", 0.2)
                        )
                        st.session_state.ai_last["followups"].append(
                            {"q": followup, "a": follow_response}
                        )
                    except Exception as exc:
                        st.error(f"è¿½é—®å¤±è´¥: {exc}")
                        return

        if st.session_state.ai_last["followups"]:
            st.markdown("#### ğŸ§µ è¿½é—®è®°å½•")
            for item in st.session_state.ai_last["followups"][-5:]:
                st.markdown(f"**Q**: {item['q']}")
                st.markdown(f"**A**: {item['a']}")

    if st.session_state.ai_history:
        with st.expander("ğŸ—‚ï¸ å†å²è§£è¯»", expanded=False):
            for item in reversed(st.session_state.ai_history[-5:]):
                stock_label = f"{item.get('stock_code', '')} {item.get('stock_name', '')}".strip()
                date_label = item.get("actual_date") or item.get("requested_date") or "æœªçŸ¥æ—¥æœŸ"
                st.markdown(
                    f"**{item['ts']} | {item['focus']} | {item['style']} | "
                    f"{item.get('advice_mode', '')} | {item.get('preset_mode', '')} | "
                    f"{stock_label or 'æœªçŸ¥æ ‡çš„'} | "
                    f"{date_label} | ä¼šè¯ {item.get('session_id', '--')}**"
                )
                if item.get("user_question"):
                    st.caption(f"è¡¥å……é—®é¢˜: {item['user_question']}")
                st.write(item["response"])


def _build_context(
    news_df: Optional[pd.DataFrame] = None,
    include_news: bool = False
) -> Dict:
    df = st.session_state.df
    analysis = st.session_state.all_analysis
    quality = st.session_state.quality_report
    stock_code = st.session_state.get("last_stock_code", "")

    stock_provider = get_stock_provider()
    stock_name = stock_code
    try:
        res = stock_provider.search(stock_code, limit=1)
        if not res.empty:
            stock_name = res.iloc[0]["åç§°"]
    except Exception:
        pass

    actual_date = df.attrs.get("actual_date")
    requested_date = df.attrs.get("requested_date")

    tick_context = st.session_state.get("tick_context")
    daily_window = 20
    analysis_day = _parse_date_value(actual_date or requested_date) or datetime.now().date()
    daily_df = _load_daily_history(stock_code, analysis_day, daily_window)
    exclude_partial_daily = (
        analysis_day == datetime.now().date() and datetime.now().time() < time(15, 5)
    )
    if exclude_partial_daily:
        daily_df = _drop_partial_daily(daily_df, analysis_day)
    daily_series = _build_daily_series(daily_df, daily_window)
    daily_trend = _build_daily_trend(daily_df, daily_window)
    if daily_trend:
        daily_trend["partial_excluded"] = exclude_partial_daily
    today_partial = _build_today_partial(
        df=st.session_state.df,
        timeseries=analysis.get("timeseries", {}),
        indicators=analysis.get("indicators", {}),
        tick_context=tick_context,
        analysis_day=analysis_day,
    )

    flows = analysis.get("flows", {})
    timeseries = analysis.get("timeseries", {})
    indicators = analysis.get("indicators", {})
    anomalies = analysis.get("anomalies", {})
    tick_available = bool(tick_context and tick_context.get("flow_summary"))
    if tick_available:
        flows = tick_context["flow_summary"]

    large_order_count = anomalies.get("summary", {}).get("large_order_count", 0)
    if tick_available:
        large_order_count = flows.get("large_order_count", large_order_count)

    top_orders = []
    if tick_context and tick_context.get("large_orders_top5"):
        for o in tick_context["large_orders_top5"]:
            top_orders.append(
                {
                    "time": str(o.get("time", "")),
                    "amount": float(o.get("amount", 0)),
                    "price": float(o.get("price", 0)),
                    "type": o.get("type", "æœªçŸ¥"),
                    "ratio": float(o.get("ratio", 0)),
                }
            )
    else:
        large_orders = anomalies.get("large_orders", [])
        large_orders_sorted = sorted(large_orders, key=lambda x: x.get("amount", 0), reverse=True)
        top_orders = [
            {
                "time": str(o.get("time", "")),
                "amount": float(o.get("amount", 0)),
                "price": float(o.get("price", 0)),
                "type": o.get("type", "æœªçŸ¥"),
                "ratio": float(o.get("ratio", 0)),
            }
            for o in large_orders_sorted[:3]
        ]

    flow_block = {
        "large_net": flows.get("large_order_net_inflow"),
        "retail_net": flows.get("retail_net_inflow"),
        "large_ratio": flows.get("large_order_ratio"),
        "large_buy": flows.get("large_buy_amount"),
        "large_sell": flows.get("large_sell_amount"),
        "quality": flows.get("flow_quality", {}),
    }
    if tick_available:
        flow_block.update(
            {
                "trade_count": flows.get("trade_count"),
                "buy_count": flows.get("buy_count"),
                "sell_count": flows.get("sell_count"),
                "neutral_count": flows.get("neutral_count"),
                "buy_amount": flows.get("buy_amount"),
                "sell_amount": flows.get("sell_amount"),
                "net_inflow": flows.get("net_inflow"),
                "buy_ratio": flows.get("buy_ratio"),
                "sell_ratio": flows.get("sell_ratio"),
                "ofi": flows.get("ofi"),
            }
        )

    tick_window_series = []
    window_minutes = None
    if tick_available:
        tick_window_series, window_minutes = _build_tick_window_series(tick_context, limit=40)

    data_scope = {
        "date": actual_date or requested_date,
        "source": "tick" if tick_available else "minute",
        "tick_available": tick_available,
        "window_minutes": window_minutes,
        "market_hours_only": True if tick_available else None,
        "quality_flags": tick_context.get("quality_flags", []) if tick_available else [],
        "daily_window_days": daily_window,
        "daily_partial_excluded": exclude_partial_daily,
    }

    context = {
        "data_scope": data_scope,
        "stock": {
            "code": stock_code,
            "name": stock_name,
            "requested_date": requested_date,
            "actual_date": actual_date,
            "data_quality_score": quality.get("quality_score", 0),
        },
        "price": {
            "open": timeseries.get("open_price"),
            "close": timeseries.get("close_price"),
            "high": timeseries.get("high_price"),
            "low": timeseries.get("low_price"),
            "change": timeseries.get("price_change"),
            "change_pct": timeseries.get("price_change_pct"),
            "amplitude": timeseries.get("amplitude"),
        },
        "liquidity": {
            "turnover_total": timeseries.get("turnover_total"),
            "volume_total": timeseries.get("volume_total"),
            "avg_price": timeseries.get("avg_price"),
        },
        "flow": flow_block,
        "indicators": {
            "vwap": indicators.get("vwap"),
            "price_vs_vwap": indicators.get("price_vs_vwap"),
            "ma5": indicators.get("ma5"),
            "ma10": indicators.get("ma10"),
            "is_above_vwap": indicators.get("is_above_vwap"),
            "is_above_ma5": indicators.get("is_above_ma5"),
            "is_above_ma10": indicators.get("is_above_ma10"),
        },
        "anomalies": {
            "large_order_count": large_order_count,
            "price_spike_count": anomalies.get("summary", {}).get("price_spike_count", 0),
            "volume_surge_count": anomalies.get("summary", {}).get("volume_surge_count", 0),
            "top_large_orders": top_orders,
        },
        "daily_series": daily_series,
        "daily_trend": daily_trend,
        "today_partial": today_partial,
    }
    if tick_context:
        tick_summary = tick_context.get("tick_ai_summary", {})
        if tick_summary:
            context["tick_summary"] = tick_summary
        if tick_window_series:
            context["tick_window_series"] = tick_window_series
        context["tick_meta"] = {
            "quality_flags": tick_context.get("quality_flags", []),
            "burst_windows": tick_context.get("burst_windows", []),
            "anomaly_notes": tick_context.get("anomaly_notes", []),
            "auction_summary": tick_context.get("auction_summary", {}),
            "volume_unit": tick_context.get("volume_unit"),
            "inferred_ratio": tick_context.get("inferred_ratio"),
        }
    if include_news:
        context["news"] = _build_news_payload(news_df, stock_name)
    return context


def _build_prompts(
    context: Dict,
    focus: str,
    style: str,
    advice_mode: str,
    preset_mode: str,
    only_data: bool,
    highlight_numbers: bool,
    add_watchlist: bool,
    user_question: str
) -> Tuple[str, str]:
    constraints = []
    if advice_mode == "åˆ†ææ¨¡å¼":
        constraints.append("ä¸è¾“å‡ºè¡ŒåŠ¨å»ºè®®ï¼Œä»…åšåˆ†æ")
    elif advice_mode == "ç»“è®ºæ¨¡å¼":
        constraints.append("å¯ç»™å‡ºæ–¹å‘æ€§ç»“è®ºï¼Œä½†ä¸ç»™ç›´æ¥æ“ä½œæŒ‡ä»¤")
    else:
        constraints.append("å…è®¸ç»™å‡ºæ¡ä»¶è§¦å‘å»ºè®®ï¼Œä¸åšæ”¶ç›Šæ‰¿è¯º")
    if only_data:
        constraints.append("ä»…åŸºäºæä¾›çš„æ•°æ®è¿›è¡Œåˆ¤æ–­ï¼Œä¸è¦ç¼–é€ ")
    if highlight_numbers:
        constraints.append("å¿…é¡»å¼•ç”¨å…³é”®æ•°å€¼ä½œä¸ºä¾æ®")
    if add_watchlist:
        constraints.append("ç»™å‡ºå¯è§‚å¯Ÿçš„è§¦å‘æ¡ä»¶æˆ–å…³é”®å˜é‡")
    constraints.append("å¿…é¡»å¼•ç”¨ daily_trend æˆ– daily_series çš„å…³é”®æ•°å€¼ä½œä¸ºè¶‹åŠ¿ä¾æ®")
    if not context.get("daily_series"):
        constraints.append("è‹¥æ—¥çº¿æ•°æ®ä¸ºç©ºï¼Œéœ€æ˜ç¡®è¯´æ˜è¶‹åŠ¿ä¾æ®ä¸è¶³")
    if context.get("today_partial", {}).get("scope", {}).get("is_partial"):
        constraints.append("today_partial ä¸ºç›˜ä¸­å¿«ç…§ï¼Œä¸èƒ½ä¸æ—¥çº¿é‡èƒ½ç›´æ¥å¯¹æ¯”")
    if preset_mode == "range":
        constraints.append("å¿…é¡»ç»™å‡ºæ”¯æ’‘/ä¸­æ¢/å‹åŠ›åŒºé—´ï¼Œå¹¶è¯´æ˜ä¾æ®")
        constraints.append("ä¼˜å…ˆä½¿ç”¨ daily_trend çš„ range_support/range_mid/range_resistance/atr_14")
    news_payload = context.get("news")
    if news_payload is not None:
        if news_payload.get("has_news"):
            constraints.append("å¦‚æœ‰æ–°é—»æ¡ç›®ï¼Œä»…åŸºäºæ–°é—»å†…å®¹æ¨æ–­æ½œåœ¨å½±å“ï¼Œé¿å…å¤¸å¤§")
        else:
            constraints.append("è‹¥æœªæä¾›æ–°é—»æ•°æ®éœ€æ˜ç¡®è¯´æ˜æ— æ³•åˆ¤æ–­æ–°é—»å½±å“")

    style_map = {
        "ç®€æ´": "4-6æ¡è¦ç‚¹ï¼Œå¥å­çŸ­",
        "ä¸“ä¸š": "åˆ†å°æ ‡é¢˜+è¦ç‚¹",
        "äº¤æ˜“å‘˜é£æ ¼": "å¼ºè°ƒç›˜ä¸­èŠ‚å¥ã€èµ„é‡‘æ–¹å‘ï¼Œè¯­æ°”ç´§å‡‘"
    }

    focus_map = {
        "èµ„é‡‘æµå‘è§£è¯»": [
            "ä¸»åŠ›/æ•£æˆ·å‡€æµå…¥æ–¹å‘ä¸å¼ºåº¦",
            "å¤§å•å æ¯”ä¸ä¸»åŠ›ä¹°å–é¢å·®å¼‚",
            "ç´¯è®¡å‡€æµå…¥æ˜¯å¦æŒç»­"
        ],
        "ç›˜ä¸­è¶‹åŠ¿ä¸èŠ‚å¥": [
            "å¼€æ”¶/é«˜ä½ä½ç½®ä¸æŒ¯å¹…",
            "VWAP/å‡çº¿åç¦»ä¸ç›˜ä¸­èŠ‚å¥",
            "ä¸Šæ¶¨åˆ†é’Ÿå æ¯”"
        ],
        "é£é™©ä¸å¼‚åŠ¨": [
            "ä»·æ ¼è·³è·ƒæ¬¡æ•°ä¸æ–¹å‘",
            "æˆäº¤é‡å¼‚å¸¸æ”¾å¤§",
            "å¤§å•å¼‚å¸¸é›†ä¸­æ—¶æ®µ"
        ],
        "ä¸»åŠ›è¡Œä¸ºå¤ç›˜": [
            "ä¸»åŠ›å‡€æµå…¥ä¸ä»·æ ¼èµ°åŠ¿æ˜¯å¦ä¸€è‡´",
            "ä¸»åŠ›ä¹°å–é¢å·®å¼‚",
            "ä¸»åŠ›å æ¯”ä¸å…³é”®æ—¶æ®µ"
        ],
    }

    system_prompt = (
        "ä½ æ˜¯ä¸“æ³¨äºAè‚¡èµ„é‡‘æµä¸è¶‹åŠ¿åˆ¤æ–­çš„åŠ©æ‰‹ï¼Œåªèƒ½å›´ç»•äº¤æ˜“ä¸é‡‘èè¯é¢˜å›ç­”ã€‚"
        "å›å¤å¿…é¡»ç»“æ„åŒ–ï¼Œè¯­è¨€ç®€æ´ï¼Œé¿å…å‘æ•£ã€‚"
    )

    output_format = [
        "è¶‹åŠ¿ä¾æ®(æ—¥çº¿/å‡çº¿/å›æ’¤/é‡èƒ½ç­‰)",
        "ç›˜ä¸­ä¾æ®(èµ„é‡‘æµ/èŠ‚å¥/å¼‚åŠ¨ç­‰)",
        "ç»“è®º(æ–¹å‘ä¸å¼ºå¼±)",
        "é£é™©/ä¸ç¡®å®šæ€§",
        "æ¡ä»¶è§¦å‘å»ºè®®(è‹¥/åˆ™ï¼Œä¸åšæ”¶ç›Šæ‰¿è¯º)",
    ]
    if preset_mode == "range":
        output_format.insert(1, "ä»·æ ¼åŒºé—´å»ºè®®(æ”¯æ’‘/ä¸­æ¢/å‹åŠ›)")

    user_prompt = {
        "åˆ†æç›®æ ‡": focus,
        "è¾“å‡ºé£æ ¼": style_map.get(style, style),
        "è¾“å‡ºæ¨¡å¼": advice_mode,
        "çº¦æŸ": constraints,
        "é‡ç‚¹å…³æ³¨": focus_map.get(focus, []),
        "è¡¥å……é—®é¢˜": user_question or "æ— ",
        "æ•°æ®å¿«ç…§": _json_safe(context),
        "è¾“å‡ºæ ¼å¼": output_format,
    }

    return system_prompt, json.dumps(user_prompt, ensure_ascii=False, indent=2)


def _summarize_settings(
    focus: str,
    style: str,
    advice_mode: str,
    preset_mode: str,
    only_data: bool,
    highlight_numbers: bool,
    add_watchlist: bool,
    user_question: str
) -> List[str]:
    tags = [focus, style, advice_mode, preset_mode]
    if only_data:
        tags.append("ä»…åŸºäºæ•°æ®")
    if highlight_numbers:
        tags.append("å¼ºè°ƒæ•°å€¼")
    if add_watchlist:
        tags.append("ç»™è§‚å¯Ÿæ¸…å•")
    if user_question:
        tags.append("å«è¡¥å……é—®é¢˜")
    return tags


def _build_followup_prompt(
    context: Dict,
    focus: str,
    constraints: List[str],
    previous_answer: str,
    followup: str
) -> str:
    focus_map = {
        "èµ„é‡‘æµå‘è§£è¯»": ["ä¸»åŠ›/æ•£æˆ·å‡€æµå…¥", "ç´¯è®¡å‡€æµå…¥", "å¤§å•å æ¯”"],
        "ç›˜ä¸­è¶‹åŠ¿ä¸èŠ‚å¥": ["ç›˜ä¸­èŠ‚å¥", "VWAP/å‡çº¿åç¦»", "æŒ¯å¹…"],
        "é£é™©ä¸å¼‚åŠ¨": ["ä»·æ ¼è·³è·ƒ", "æˆäº¤é‡æ¿€å¢", "å¤§å•å¼‚å¸¸"],
        "ä¸»åŠ›è¡Œä¸ºå¤ç›˜": ["ä¸»åŠ›å‡€æµå…¥ä¸ä»·æ ¼ä¸€è‡´æ€§", "ä¸»åŠ›ä¹°å–é¢å·®å¼‚"],
    }
    payload = {
        "ä»»åŠ¡": "åŸºäºå·²æœ‰è§£è¯»ç»§ç»­å›ç­”è¿½é—®ï¼Œä¿æŒé‡‘èäº¤æ˜“è¯­å¢ƒ",
        "åˆ†æç›®æ ‡": focus,
        "çº¦æŸ": constraints,
        "é‡ç‚¹å…³æ³¨": focus_map.get(focus, []),
        "å·²æœ‰è§£è¯»": previous_answer,
        "è¿½é—®": followup,
        "æ•°æ®å¿«ç…§": context,
        "è¾“å‡ºè¦æ±‚": [
            "ç›´æ¥å›ç­”é—®é¢˜",
            "å¼•ç”¨å…³é”®æ•°æ®",
            "ä¸æ‰©å±•åˆ°æ— å…³è¯é¢˜"
        ],
    }
    return json.dumps(payload, ensure_ascii=False, indent=2)


def _json_safe(value):
    if isinstance(value, dict):
        return {str(k): _json_safe(v) for k, v in value.items()}
    if isinstance(value, list):
        return [_json_safe(v) for v in value]
    if isinstance(value, (datetime, pd.Timestamp)):
        return value.isoformat()
    if isinstance(value, (str, int, float, bool)) or value is None:
        return value
    if hasattr(value, "item"):
        try:
            return value.item()
        except Exception:
            pass
    try:
        if pd.isna(value):
            return None
    except Exception:
        pass
    return str(value)
